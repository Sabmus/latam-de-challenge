{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LATAM Data Engineer Challenge\n",
        "---\n",
        "Hola! gracias por leer!  \n",
        "A continuación paso a detallar los pasos realizados y todo lo pensado para este desafio.\n",
        "\n",
        "> #### Tech usadas  \n",
        "> - PySpark v3.5.1  \n",
        "> - Python v3.9.13\n",
        "\n",
        "### Proceso mental\n",
        "Bueno, después de revisar la estructura del dataset y revisar la documentación, pensé que lo primero que debía hacer era buscar una manera de sacar todos  \n",
        "los **quotedTweets** ya que finalmente correpsonden a otro tweet, por ende mi idea era también tenerlos en la data para procesar todo. Y quité los duplicados por **id**.  \n",
        "\n",
        "Siguiendo, y como quería usar pyspark creé una clase que instanciara a **Spark** para dejar ahí toda la configuración y cualquier método que me fuera útil.  \n",
        "Caba descatar que todo el proceso lo corrí en local, por ende fui probando distintas configuraciones de **Spark**, según ciertos errores que me iban arrojando las ejecuciones de código.  \n",
        "\n",
        "Principalmente sufrí con un par de errores del tipo: \"_Py4JJavaError: An error occurred while calling {ABC}_\". Esto ocurría casi todas las veces que usé UDFs,  \n",
        "busqué varias soluciones pero no dí con ninguna así que opté por no usar UDFs.  \n",
        "\n",
        "\n",
        "  \n",
        "   \n",
        "referencias\n",
        "- [Instalar PySpark en Windows](https://medium.com/@dipan.saha/getting-started-with-pyspark-day-1-37e5e6fdc14b)\n",
        "- [Documentación de PySpark config](https://spark.apache.org/docs/3.0.2/configuration.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero inicializo los nombres de archivos y las rutas, además de preguntar si es que existe el .zip para descomprimirlo en la carpeta **data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cProfile\n",
        "import pstats\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# declaro nombres de archivo y rutas\n",
        "json_file = \"farmers-protest-tweets-2021-2-4.json\"\n",
        "parquet_file = \"tweets.parquet\"\n",
        "data_folder_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data'))\n",
        "json_path = f\"{data_folder_path}\\\\{json_file}\"\n",
        "parquet_path = f\"{data_folder_path}\\\\{parquet_file}\"\n",
        "\n",
        "# checkea si existe la carpeta data, sino, la crea\n",
        "if not os.path.exists(data_folder_path):\n",
        "    os.makedirs(data_folder_path)\n",
        "\n",
        "# checkea si existe el archivo tweets.json.zip, si no, checkea si existe el archivo json, si no, lanza excepción\n",
        "if not os.path.exists(f\"{data_folder_path}\\\\tweets.json.zip\"):\n",
        "    if not os.path.exists(f\"{data_folder_path}\\\\{json_file}\"):\n",
        "        raise FileNotFoundError(\"Por favor descargue el archivo 'tweets.json.zip' y colóquelo en la carpeta 'data'\")\n",
        "\n",
        "# si no existe el archivo json, descomprime el archivo zip\n",
        "if not os.path.exists(f\"{data_folder_path}\\\\{json_file}\"):\n",
        "    with ZipFile(f\"{data_folder_path}\\\\tweets.json.zip\", 'r') as zObject: \n",
        "        zObject.extractall(path=f\"{data_folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Luego pregunta si es que existe el archivo parquet, sino, lo crea  \n",
        "\n",
        "La función **json_to_parquet** toma el json principal (el de 389mb) y lo \"desnestea\". Esto va a buscar si existe algún elemento **quotedTweet**,  \n",
        "y si existe une sus datos con el dataframe principal, esto para crear un gran dataset con todos los tweets, luego de ellos lo pasa a formato **parquet**.  \n",
        "\n",
        "```python\n",
        "def json_to_parquet(ruta_del_json: str, ruta_del_parquet: str) -> None\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from json_to_parquet import json_to_parquet\n",
        "# si no encuentra el archivo parquet, lo crea\n",
        "if not os.path.exists(f\"{parquet_path}\"):\n",
        "    json_to_parquet(json_path, parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 1\n",
        "Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "```\n",
        "```python\n",
        "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(datetime.date(1999, 11, 15), \"LATAM321\"), (datetime.date(1999, 7, 15), \"LATAM_CHI\"), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''from q1_memory import q1_memory\n",
        "q1_mem_result = q1_memory(f\"{parquet_path}\")\n",
        "q1_mem_result'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''from q1_time import q1_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q1_time_result = q1_time(f\"{parquet_path}\")\n",
        "\n",
        "q1_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q1_stats.dump_stats('./stats/q1_time')\n",
        "q1_stats.print_stats()\n",
        "q1_time_result'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 2\n",
        "Los top 10 emojis más usados con su respectivo conteo. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(\"✈️\", 6856), (\"❤️\", 5876), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''from q2_memory import q2_memory\n",
        "q2_mem_result = q2_memory(f\"{parquet_path}\")\n",
        "q2_mem_result'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from q2_time import q2_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q2_time_result = q2_time(f\"{parquet_path}\")\n",
        "\n",
        "q2_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q2_stats.dump_stats('./stats/q2_time')\n",
        "q2_stats.print_stats()\n",
        "q2_time_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 3\n",
        "El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(\"LATAM321\", 387), (\"LATAM_CHI\", 129), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''from q3_memory import q3_memory\n",
        "q3_mem_result = q3_memory(f\"{data_folder_path}/{file_path}\")\n",
        "q3_mem_result'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''from q3_time import q3_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q3_time_result = q3_time(f\"{data_folder_path}/{file_path}\")\n",
        "\n",
        "q3_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q3_stats.dump_stats('./stats/q3_time')\n",
        "q3_stats.print_stats()\n",
        "q3_time_result'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

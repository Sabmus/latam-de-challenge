{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 12px\">En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluci√≥n y todas las suposiciones que est√°s considerando. Aqu√≠ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc.</p>\n",
        "\n",
        "<div hidden>\n",
        "    <!-- Font normal -->\n",
        "    <p style=\"font-size: 16px\">text</p>\n",
        "    <!-- Font peque√±o -->\n",
        "    <p style=\"font-size: 15px\">text</p>\n",
        "    <!-- hr interno -->\n",
        "    <hr style=\"height: 1px; color: hsl(221.25 24.24% 18%); background: hsl(221.25 24.24% 18%); font-size: 0; border: 0\">\n",
        "    <!-- hr externo -->\n",
        "    <hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">\n",
        "    <b style=\"color: hsl(163.3 98.98% 58.43%)\">bold</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 align=\"center\">LATAM Data Engineer Challenge</h1>\n",
        "<hr style=\"height: 1px; color: hsl(221.25 24.24% 18%); background: hsl(221.25 24.24% 18%); font-size: 0; border: 0\">\n",
        "<p style=\"font-size: 16px\">Hola! gracias por leer! A continuaci√≥n paso a detallar los pasos realizados y todo lo pensado para este desafio.</p>\n",
        "\n",
        "<div style=\"background-color: hsl(221.25 24.24% 18%); width: 30%; padding:5px 10px; border-radius: 5px\">\n",
        "<h3>Tech usadas</h3>\n",
        "<ul>\n",
        "    <li>PySpark v3.5.1</li>\n",
        "    <li>Python v3.9.13</li>\n",
        "    <li>Pandas v2.2.2</li>\n",
        "    <li>Java version \"1.8.0_381\"</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "\n",
        "<h2>Proceso mental</h2>\n",
        "<p style=\"font-size: 16px\">Despu√©s de revisar la estructura del dataset y revisar la documentaci√≥n, pens√© que lo primero que deb√≠a hacer era buscar una manera de unir todos los <b style=\"color: hsl(163.3 98.98% 58.43%)\">quotedTweets</b> ya que finalmente corresponden a otro tweet, y con esto lograr un mayor dataset.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">Siguiendo, y como quer√≠a usar PySpark cre√© una clase que instanciara a <b style=\"color: hsl(163.3 98.98% 58.43%)\">Spark</b> para dejar ah√≠ toda la configuraci√≥n y cualquier m√©todo que me fuera √∫til. Fui probando distintas configuraciones de <b style=\"color: hsl(163.3 98.98% 58.43%)\">Spark</b> seg√∫n avanzaba en el desafio, y seg√∫n errores que iba encontrando.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">Principalmente sufr√≠ con un par de errores del tipo: \"<i>Py4JJavaError: An error occurred while calling {ABC}</i>\". Esto ocurr√≠a todas las veces que us√© UDFs, busqu√© varias soluciones pero ninguna me sirvi√≥ as√≠ que opt√© por no usar UDFs. Pienso que probablemente tengo algo mal seteado en Spark, y por ello me arroj√≥ error cada vez que quise usarlas.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">Luego de realizar algunas pruebas con el archivo JSON, prefer√≠ pasarlo a Parquet para as√≠ mejorar el rendimiento de las ejecuciones.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">En cuanto a las respuestas de Tiempo us√© <b style=\"color: hsl(163.3 98.98% 58.43%)\">Pandas</b>. Al principio pens√© en usar PySpark para todo pero como corr√≠a bastante bien en tiempo, pens√© que finalmente seg√∫n se pueda tunear Spark, se puede lograr un buen tiempo de ejecuci√≥n as√≠ como de manejo de memor√≠a. Asi que para variar tambi√©n mis respuestas me fui por ese camino.</p>\n",
        "\n",
        "<h2>Mejoras</h2>\n",
        "<p style=\"font-size: 16px\">Con respecto a las mejoras se me ocurren varias cosas: </p>\n",
        "<ol>\n",
        "    <li style=\"font-size: 16px\">Leyendo algunos art√≠culos sobre optimizaci√≥n, creo que se podr√≠a utilizar Bucketing, Partitioning y Z-Ordering:</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">Partitioning: en el sentido de crear particiones usando alguna columna en particular seg√∫n se necesite: 'df.write.partitionBy(\"column_name\")'</li>\n",
        "            <li style=\"font-size: 16px\">Bucketing: con esto se puede agrupar data en \"buckets\" del mismo tama√±o, lo cual ayuda cuando se usa groupBy, joins y orderBy</li>\n",
        "            <li style=\"font-size: 16px\">Z-Ordering: para ordernar espacialmente la data y juntar data relacionada.</li>\n",
        "        </ul>\n",
        "    <li style=\"font-size: 16px\">Reducir a√∫n m√°s el dataset. Para las 3 preguntas solo se necesitaban 5 columnas: \"id\", \"date\", \"user.username\", \"mentionedUser[].username\" y \"content\":</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">teniendo un dataset m√°s peque√±o har√≠a que mejora el rendimiento en el sentido de la carga inicial de la data.</li>\n",
        "        </ul>\n",
        "    <li style=\"font-size: 16px\">Trabajar a√∫n mas la columna \"content\":</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">Para la pregunta 2, como finalmente solo se quer√≠an contar emojis lo ideal ser√≠a solo dejar los car√°cteres de emojis. La forma que us√© para esto fue quitar casi todo pero me quedaron car√°cteres de lenguajes de la India.</li>\n",
        "        </ul>\n",
        "    <li style=\"font-size: 16px\">Pensando en Spark, idea contar con mayor cantidad de cl√∫sters, seg√∫n se necesite. Esto aporta escalabilidad seg√∫n crezcan los datos.</li>\n",
        "    <li style=\"font-size: 16px\">Subir todo a la nube:</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">Subir los archivos .py a un bucket en GCP</li>\n",
        "            <li style=\"font-size: 16px\">Luego con el ID de los archivos en el bucket, usarlos para crear Job en Dataproc</li>\n",
        "            <li style=\"font-size: 16px\">Ac√° podr√≠amos dejar la data en BigQuery para luego procesarla con alguna herramienta visual</li>\n",
        "            <li style=\"font-size: 16px\">Todo esto se podr√≠a orquestar con Airflow y as√≠ monitorear las ejecuciones</li>\n",
        "        </ul>\n",
        "</ol>\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"height: 1px; color: hsl(221.25 24.24% 18%); background: hsl(221.25 24.24% 18%); font-size: 0; border: 0\">\n",
        "\n",
        "<p style=\"font-size: 14px\">Referencias</p>\n",
        "<ul>\n",
        "    <li><a href=\"https://medium.com/@dipan.saha/getting-started-with-pyspark-day-1-37e5e6fdc14b\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Instalar PySpark en windows</a></li>\n",
        "    <li><a href=\"https://spark.apache.org/docs/3.0.2/configuration.html\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Docs de configuraci√≥n de PySpark</a></li>\n",
        "    <li><a href=\"https://medium.com/@shilpajoshi1635/optimizing-query-performance-in-pyspark-with-partitioning-bucketing-and-z-ordering-a5e215e6c272\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Partitioning, Bucketing y Z-ordering</a></li>\n",
        "    <li><a href=\"https://www.freecodecamp.org/news/what-is-google-dataproc/\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Usemos Dataproc!</a></li>\n",
        "    <li><a href=\"https://www.youtube.com/live/R2c6sZYahe0?t=1126s\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">YT sobre pipelines usando PySpark y Airflow con GCP</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Inicializaci√≥n de variables</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 16px\">Cre√© la celda siguiente para chequear la instalaci√≥n de Java.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check de java\n",
        "from subprocess import check_output, STDOUT\n",
        "try:\n",
        "  check_output(\"java -version\", stderr=STDOUT, shell=True).decode('utf-8')\n",
        "except OSError:\n",
        "  # raise KeyboardInterrupt(\"Por favor instale Java JDK para continuar.\")\n",
        "  print(\"Java no est√° instalado en su PC, para que el programa funcione correctamente, por favor instale Java en su PC.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 16px\">Luego inicializo los nombres de archivos y las rutas, adem√°s de preguntar si es que existe el .zip para descomprimirlo en la carpeta <b style=\"color: hsl(163.3 98.98% 58.43%)\">data</b></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cProfile\n",
        "import pstats\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# declaro nombres de archivo y rutas\n",
        "json_file = \"farmers-protest-tweets-2021-2-4.json\"\n",
        "parquet_file = \"tweets.parquet\"\n",
        "data_folder_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data'))\n",
        "json_path = f\"{data_folder_path}\\\\{json_file}\"\n",
        "parquet_path = f\"{data_folder_path}\\\\{parquet_file}\"\n",
        "\n",
        "# max lineas a imprimir en stats\n",
        "max_print_stat = 100\n",
        "\n",
        "# checkea si existe la carpeta data, sino, la crea\n",
        "if not os.path.exists(data_folder_path):\n",
        "    os.makedirs(data_folder_path)\n",
        "\n",
        "# checkea si existe el archivo tweets.json.zip, si no, checkea si existe el archivo json, si no, lanza excepci√≥n\n",
        "if not os.path.exists(f\"{data_folder_path}\\\\tweets.json.zip\"):\n",
        "    if not os.path.exists(f\"{data_folder_path}\\\\{json_file}\"):\n",
        "        raise FileNotFoundError(\"Por favor descargue el archivo 'tweets.json.zip' y col√≥quelo en la carpeta 'data'\")\n",
        "\n",
        "# si no existe el archivo json, descomprime el archivo zip\n",
        "if not os.path.exists(f\"{data_folder_path}\\\\{json_file}\"):\n",
        "    with ZipFile(f\"{data_folder_path}\\\\tweets.json.zip\", 'r') as zObject: \n",
        "        zObject.extractall(path=f\"{data_folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 16px\">Luego pregunta si es que existe el archivo parquet, sino, lo crea.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">La funci√≥n <b style=\"color: hsl(163.3 98.98% 58.43%)\">json_to_parquet</b> toma el json principal (el de 389mb) y lo \"desnestea\". Esto va a buscar si existe alg√∫n elemento <b style=\"color: hsl(163.3 98.98% 58.43%)\">quotedTweet</b>, y si existe une sus datos con el dataframe principal, esto para crear un gran dataset con todos los tweets, luego de ellos lo pasa a formato <b style=\"color: hsl(163.3 98.98% 58.43%)\">parquet</b>.</p>\n",
        "\n",
        "```python\n",
        "def json_to_parquet(ruta_del_json: str, ruta_del_parquet: str) -> None\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from helpers.helperFuncs import json_to_parquet\n",
        "# si no encuentra el archivo parquet, lo crea\n",
        "if not os.path.exists(f\"{parquet_path}\"):\n",
        "    json_to_parquet(json_path, parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 1\n",
        "Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "```\n",
        "```python\n",
        "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(datetime.date(1999, 11, 15), \"LATAM321\"), (datetime.date(1999, 7, 15), \"LATAM_CHI\"), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">Creo que las mejoras que se pueden hacer ac√° es no utilizar \"limit\", y trabajar sobre las listas resultantes usando \"take\". Le√≠ que limit se hace en dos pasos, uno local y otro global, este √∫ltimo genera un shuffle lo cual es costoso en dataset grandes. <a href=\"https://towardsdatascience.com/stop-using-the-limit-clause-wrong-with-spark-646e328774f5\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Referencia</a></p>\n",
        "\n",
        "```python\n",
        " # obtengo las top 10 fechas con mas tweets \n",
        "top_10_dates = df.groupBy(df[\"date_only\"].alias(\"date\")) \\\n",
        "    .agg(sf.count(\"id\").alias(\"tweetCount\")) \\\n",
        "    .orderBy(sf.desc(\"tweetCount\")) \\\n",
        "    .limit(10).cache()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: f:\\GitHub\\DataEngineer\\latam_de_challenge\\src\\q1_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     8     73.2 MiB     73.2 MiB           1   @profile\n",
            "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "    10                                             # Inicializacion de Spark\n",
            "    11     75.5 MiB      2.3 MiB           1       spark = SparkClass(\"Q1: Memory\")\n",
            "    12                                             # Carga de datos\n",
            "    13     75.5 MiB      0.0 MiB           1       df = spark.load_parquet(file_path).select(\"id\", \"username\", \"date\")\n",
            "    14     75.6 MiB      0.1 MiB           3       df = df.withColumn(\"date_only\", sf.col(\"date\").cast(DateType())) \\\n",
            "    15     75.5 MiB      0.0 MiB           1           .repartition(4, \"date_only\") \\\n",
            "    16     75.5 MiB      0.0 MiB           1           .drop(\"date\").cache()\n",
            "    17                                         \n",
            "    18                                             # obtengo las top 10 fechas con mas tweets \n",
            "    19     75.6 MiB      0.0 MiB           4       top_10_dates = df.groupBy(df[\"date_only\"].alias(\"date\")) \\\n",
            "    20     75.6 MiB      0.0 MiB           1           .agg(sf.count(\"id\").alias(\"tweetCount\")) \\\n",
            "    21     75.6 MiB      0.0 MiB           1           .orderBy(sf.desc(\"tweetCount\")) \\\n",
            "    22     75.6 MiB      0.0 MiB           1           .limit(10).cache()\n",
            "    23                                             \n",
            "    24                                             # filtro el df principal con las top 10 fechas para luego agrupar seg√∫n cantidad de tweets por usuario\n",
            "    25     76.2 MiB      0.6 MiB          13       filtro = df.date_only.isin([row.date for row in top_10_dates.take(10)])\n",
            "    26     76.3 MiB      0.1 MiB           5       top_user_by_date = df.filter(filtro) \\\n",
            "    27     76.2 MiB      0.0 MiB           1           .groupBy(df[\"date_only\"].alias(\"date\"), \"username\") \\\n",
            "    28     76.2 MiB      0.0 MiB           2           .agg(sf.count(\"id\").alias(\"tweetCount\")).orderBy(sf.desc(\"tweetCount\")) \\\n",
            "    29     76.2 MiB      0.0 MiB           1           .limit(10).cache()\n",
            "    30                                             \n",
            "    31                                             # libero memoria de df\n",
            "    32     76.3 MiB      0.0 MiB           1       df.unpersist()\n",
            "    33                                             \n",
            "    34                                             # hago un join con ambos df para obtener el resultado final\n",
            "    35     76.3 MiB      0.0 MiB           5       result = top_10_dates.alias(\"top_10_dates\") \\\n",
            "    36     76.3 MiB      0.0 MiB           1               .join(top_user_by_date.alias(\"top_user_by_date\"),top_10_dates.date == top_user_by_date.date, \"inner\") \\\n",
            "    37     76.3 MiB      0.0 MiB           1               .select(\"top_10_dates.date\", \"top_user_by_date.username\") \\\n",
            "    38     76.3 MiB      0.0 MiB           1               .orderBy(sf.desc(\"top_10_dates.tweetCount\")) \\\n",
            "    39     76.3 MiB      0.0 MiB           1               .take(10)\n",
            "    40                                             \n",
            "    41                                             # libero memoria de top_10_dates y top_user_by_date\n",
            "    42     76.3 MiB      0.0 MiB           1       top_10_dates.unpersist()\n",
            "    43     76.3 MiB      0.0 MiB           1       top_user_by_date.unpersist()\n",
            "    44                                         \n",
            "    45                                             # termino ejecucion de spark\n",
            "    46     76.2 MiB     -0.1 MiB           1       spark.stop_spark()\n",
            "    47     76.2 MiB      0.0 MiB          13       return [(row.date, row.username) for row in result]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
              " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
              " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
              " (datetime.date(2021, 2, 16), 'jot__b'),\n",
              " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
              " (datetime.date(2021, 2, 18), 'rebelpacifist'),\n",
              " (datetime.date(2021, 2, 15), 'jot__b'),\n",
              " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
              " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
              " (datetime.date(2021, 2, 19), 'KaurDosanjh1979')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q1_memory import q1_memory\n",
        "q1_mem_result = q1_memory(f\"{parquet_path}\")\n",
        "q1_mem_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">Ac√° (y en realidad lo mismo para el c√≥digo de memoria) creo que en vez de crear una nueva columna con la fecha, se podr√≠a dejar de antemano en el parquet, para as√≠ ahorrar la conversi√≥n.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         937513 function calls (936682 primitive calls) in 1.393 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 1132 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "    50520    0.358    0.000    0.785    0.000 multi.py:2982(get_loc)\n",
            "        1    0.247    0.247    0.247    0.247 datetimes.py:456(_array_strptime_with_fallback)\n",
            "    50549    0.134    0.000    0.134    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "   101048    0.096    0.000    0.107    0.000 base.py:3777(get_loc)\n",
            "    50521    0.085    0.000    0.219    0.000 multi.py:136(_codes_to_ints)\n",
            "        1    0.069    0.069    0.908    0.908 multi.py:2355(drop)\n",
            "        1    0.059    0.059    0.060    0.060 pandas_compat.py:755(table_to_dataframe)\n",
            "        1    0.034    0.034    0.036    0.036 multi.py:758(_values)\n",
            "        1    0.029    0.029    0.029    0.029 datetimes.py:1436(date)\n",
            "        3    0.029    0.010    0.030    0.010 algorithms.py:548(factorize_array)\n",
            "    50520    0.027    0.000    0.056    0.000 multi.py:2738(_check_indexing_error)\n",
            "   101058    0.027    0.000    0.036    0.000 multi.py:1046(nlevels)\n",
            "       12    0.019    0.002    0.019    0.002 {method 'argsort' of 'numpy.ndarray' objects}\n",
            "    50606    0.016    0.000    0.030    0.000 inference.py:334(is_hashable)\n",
            "        1    0.016    0.016    0.016    0.016 core.py:1389(read)\n",
            "153638/153378    0.015    0.000    0.015    0.000 {built-in method builtins.len}\n",
            "    50630    0.013    0.000    0.013    0.000 {built-in method builtins.hash}\n",
            "104392/104363    0.012    0.000    0.012    0.000 {built-in method builtins.isinstance}\n",
            "   101048    0.011    0.000    0.011    0.000 base.py:6672(_maybe_cast_indexer)\n",
            "       13    0.008    0.001    0.008    0.001 {built-in method _imp.create_dynamic}\n",
            "    52034    0.007    0.000    0.007    0.000 {method 'append' of 'list' objects}\n",
            "       22    0.005    0.000    0.007    0.000 take.py:120(_take_nd_ndarray)\n",
            "  108/105    0.005    0.000    0.005    0.000 {built-in method numpy.asarray}\n",
            "        1    0.005    0.005    0.009    0.009 sorting.py:687(compress_group_index)\n",
            "        1    0.005    0.005    0.005    0.005 algorithms.py:457(isin)\n",
            "     13/8    0.004    0.000    0.013    0.002 {built-in method _imp.exec_dynamic}\n",
            "       88    0.004    0.000    0.004    0.000 {built-in method nt.stat}\n",
            "        1    0.004    0.004    1.393    1.393 q1_time.py:5(q1_time)\n",
            "        6    0.002    0.000    0.002    0.000 missing.py:564(_array_equivalent_object)\n",
            "        2    0.002    0.001    0.065    0.032 ops.py:700(size)\n",
            "        8    0.002    0.000    0.002    0.000 {built-in method io.open_code}\n",
            "        8    0.002    0.000    0.003    0.000 base.py:2313(is_unique)\n",
            "        1    0.002    0.002    0.015    0.015 core.py:1243(__init__)\n",
            "        1    0.002    0.002    0.002    0.002 sorting.py:246(_decons_group_index)\n",
            "       10    0.002    0.000    0.002    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
            "       69    0.002    0.000    0.002    0.000 {built-in method numpy.empty}\n",
            "        2    0.001    0.001    0.919    0.459 selectn.py:55(nlargest)\n",
            "       33    0.001    0.000    0.001    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
            "        3    0.001    0.000    0.001    0.000 algorithms.py:280(_check_object_for_strings)\n",
            "      210    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap_external>:91(_path_join)\n",
            "        1    0.001    0.001    0.001    0.001 multi.py:1193(_engine)\n",
            "        1    0.001    0.001    0.001    0.001 sorting.py:122(get_group_index)\n",
            "        8    0.001    0.000    0.001    0.000 {built-in method marshal.loads}\n",
            "        1    0.001    0.001    0.001    0.001 {method 'put' of 'numpy.ndarray' objects}\n",
            "        3    0.001    0.000    0.050    0.017 algorithms.py:610(factorize)\n",
            "    44/11    0.001    0.000    0.002    0.000 sre_parse.py:494(_parse)\n",
            "        9    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
            "        3    0.001    0.000    0.913    0.304 generic.py:4796(_drop_axis)\n",
            "        1    0.000    0.000    0.004    0.004 datetimes.py:126(_guess_datetime_format_for_array)\n",
            "        3    0.000    0.000    0.001    0.000 utils.py:239(maybe_convert_indices)\n",
            "       15    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
            "     66/6    0.000    0.000    0.001    0.000 sre_compile.py:87(_compile)\n",
            "       52    0.000    0.000    0.006    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)\n",
            "        4    0.000    0.000    0.005    0.001 _function_base_impl.py:5335(delete)\n",
            "        8    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
            "        2    0.000    0.000    0.061    0.030 ops.py:758(_get_compressed_codes)\n",
            "        2    0.000    0.000    0.917    0.459 selectn.py:90(compute)\n",
            "       19    0.000    0.000    0.001    0.000 cast.py:1157(maybe_infer_to_datetimelike)\n",
            "        3    0.000    0.000    0.019    0.006 algorithms.py:1452(safe_sort)\n",
            "   147/19    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
            "      156    0.000    0.000    0.001    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
            "       62    0.000    0.000    0.000    0.000 base.py:649(_simple_new)\n",
            "        1    0.000    0.000    0.000    0.000 datetimelike.py:2353(copy)\n",
            "       18    0.000    0.000    0.002    0.000 base.py:475(__new__)\n",
            "        7    0.000    0.000    0.036    0.005 base.py:836(__iter__)\n",
            "       12    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
            "      871    0.000    0.000    0.000    0.000 sre_parse.py:234(__next)\n",
            "       27    0.000    0.000    0.001    0.000 construction.py:517(sanitize_array)\n",
            "      766    0.000    0.000    0.000    0.000 sre_parse.py:255(get)\n",
            "     26/5    0.000    0.000    0.026    0.005 <frozen importlib._bootstrap>:1002(_find_and_load)\n",
            "        4    0.000    0.000    0.000    0.000 {built-in method builtins.eval}\n",
            "        4    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
            "  640/636    0.000    0.000    0.029    0.000 {built-in method builtins.getattr}\n",
            "        4    0.000    0.000    0.000    0.000 __init__.py:345(namedtuple)\n",
            "       36    0.000    0.000    0.000    0.000 generic.py:6236(__finalize__)\n",
            "       25    0.000    0.000    0.007    0.000 <frozen importlib._bootstrap>:901(_find_spec)\n",
            "       48    0.000    0.000    0.000    0.000 sre_compile.py:292(_optimize_charset)\n",
            "        1    0.000    0.000    0.004    0.004 sorting.py:718(_reorder_by_uniques)\n",
            "       78    0.000    0.000    0.000    0.000 threading.py:1388(enumerate)\n",
            "       20    0.000    0.000    0.000    0.000 cast.py:551(maybe_promote)\n",
            "      462    0.000    0.000    0.000    0.000 sre_parse.py:165(__getitem__)\n",
            "       24    0.000    0.000    0.000    0.000 base.py:5323(__contains__)\n",
            "       34    0.000    0.000    0.000    0.000 numeric.py:300(full)\n",
            "       43    0.000    0.000    0.000    0.000 generic.py:278(__init__)\n",
            "       78    0.000    0.000    0.000    0.000 ipkernel.py:790(<setcomp>)\n",
            "       75    0.000    0.000    0.000    0.000 generic.py:6301(__setattr__)\n",
            "        8    0.000    0.000    0.002    0.000 series.py:389(__init__)\n",
            "      317    0.000    0.000    0.000    0.000 generic.py:42(_instancecheck)\n",
            "      712    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
            "        2    0.000    0.000    0.061    0.030 ops.py:743(group_info)\n",
            "    75/17    0.000    0.000    0.000    0.000 sre_parse.py:175(getwidth)\n",
            "      196    0.000    0.000    0.004    0.000 re.py:289(_compile)\n",
            "       13    0.000    0.000    0.000    0.000 base.py:842(_engine)\n",
            "      317    0.000    0.000    0.000    0.000 generic.py:37(_check)\n",
            "       25    0.000    0.000    0.006    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)\n",
            "      210    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:114(<listcomp>)\n",
            "      496    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
            "      546    0.000    0.000    0.000    0.000 threading.py:1109(ident)\n",
            "     25/4    0.000    0.000    0.026    0.006 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)\n",
            "        5    0.000    0.000    0.001    0.000 managers.py:708(_slice_take_blocks_ax0)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
              " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
              " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
              " (datetime.date(2021, 2, 16), 'jot__b'),\n",
              " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
              " (datetime.date(2021, 2, 18), 'rebelpacifist'),\n",
              " (datetime.date(2021, 2, 15), 'jot__b'),\n",
              " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
              " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
              " (datetime.date(2021, 2, 19), 'KaurDosanjh1979')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q1_time import q1_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q1_time_result = q1_time(f\"{parquet_path}\")\n",
        "\n",
        "q1_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q1_stats.dump_stats('./stats/q1_time')\n",
        "q1_stats.print_stats(max_print_stat)\n",
        "q1_time_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 2\n",
        "Los top 10 emojis m√°s usados con su respectivo conteo. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(\"‚úàÔ∏è\", 6856), (\"‚ù§Ô∏è\", 5876), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">Como coment√© antes, creo que se puede mejorar el patr√≥n de b√∫squeda de emojis, y el de reducci√≥n de la columna \"content\". Para esta pregunta us√© la siguiente expresi√≥n regular:</p>\n",
        "\n",
        "```shell\n",
        "[\\u0900-\\u097F]+|[\\x00-\\x7F]+\n",
        "```\n",
        "<p style=\"font-size: 16px\">La cual se encarga de quitar casi todo dejandolo solo emojis, pero si bien funciona bien, creo que se puede mejorar para quitar a√∫n mas car√°cteres y dejar solo emojis.</p>\n",
        "\n",
        "<p style=\"font-size: 14px\"><i>Nota aparte: prob√© este c√≥digo usando el patr√≥n completo de emojis sacado de la librer√≠a \"emojis\" y me di√≥ el mismo resultado sobre la columna completa sin limpiar.</i></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: f:\\GitHub\\DataEngineer\\latam_de_challenge\\src\\q2_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    11    184.1 MiB    184.1 MiB           1   @profile\n",
            "    12                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "    13                                             # Inicializacion de Spark\n",
            "    14    184.2 MiB      0.2 MiB           1       spark = SparkClass(\"Q2: Memory\")\n",
            "    15                                             # Carga de datos\n",
            "    16    184.2 MiB      0.0 MiB           1       df = spark.load_parquet(file_path).select(\"content\").where(sf.col(\"content\").isNotNull()).cache()\n",
            "    17                                             \n",
            "    18                                             # creo patron de emojis\n",
            "    19                                             # https://carpedm20.github.io/emoji/docs/index.html#regular-expression\n",
            "    20    184.2 MiB      0.0 MiB           1       emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True) \n",
            "    21    184.8 MiB      0.4 MiB       10071       pattern = '(' + '|'.join(re.escape(u) for u in emojis) + ')'\n",
            "    22                                         \n",
            "    23                                             # quise hacerlo con rdd pero no pude convertirlo a dataframe\n",
            "    24                                             # 'PipelinedRDD' object has no attribute 'toDF' in PySpark\n",
            "    25                                             # https://stackoverflow.com/questions/32788387/pipelinedrdd-object-has-no-attribute-todf-in-pyspark\n",
            "    26                                             #rdd = df.rdd.map(lambda x: (find_all_emojis(x.content))).toDF([\"content\"])\n",
            "    27                                         \n",
            "    28                                             # patr√≥n para eliminar casi todo lo que no sea emoji\n",
            "    29                                             # con esto la idea es reducir el tama√±o de \"content\", que tenga menos \n",
            "    30                                             # caracteres a procesar\n",
            "    31    184.6 MiB     -0.2 MiB           1       short_pattern = '([\\u0900-\\u097F]+|[\\x00-\\x7F]+)'\n",
            "    32    184.6 MiB      0.0 MiB           2       content = df.select(sf.regexp_replace(sf.col('content'), fr'{short_pattern}', '').alias(\"content\")) \\\n",
            "    33    184.6 MiB      0.0 MiB           1           .filter(sf.col(\"content\") != \"\").cache()\n",
            "    34                                             \n",
            "    35    184.6 MiB      0.0 MiB           1       df.unpersist()\n",
            "    36                                             \n",
            "    37                                             # extraigo emojis usando expresi√≥n regular y el patr√≥n creado\n",
            "    38    184.6 MiB      0.0 MiB           3       df2 = content.withColumn(\"emojis\", sf.expr(f\"regexp_extract_all(content, r'{pattern}')\")) \\\n",
            "    39    184.6 MiB      0.0 MiB           1           .where(sf.size(sf.col(\"emojis\")) > 0) \\\n",
            "    40    184.6 MiB      0.0 MiB           1           .drop(\"content\") \\\n",
            "    41                                                 .cache()\n",
            "    42                                             \n",
            "    43    184.6 MiB      0.0 MiB           1       content.unpersist()\n",
            "    44                                             \n",
            "    45                                             # uso explode para \"abrir\" cada lista de emojis\n",
            "    46    184.6 MiB      0.0 MiB           1       df3 = df2.select(sf.explode(\"emojis\").alias(\"emoji\")).cache()\n",
            "    47                                         \n",
            "    48    184.6 MiB      0.0 MiB           1       df2.unpersist()\n",
            "    49                                         \n",
            "    50    184.6 MiB     -0.0 MiB           4       top_10_emojis = df3.groupBy(\"emoji\") \\\n",
            "    51    184.6 MiB      0.0 MiB           1           .agg(sf.count(\"emoji\").alias(\"emojiCount\")) \\\n",
            "    52    184.6 MiB      0.0 MiB           1           .orderBy(sf.desc(\"emojiCount\")) \\\n",
            "    53    184.6 MiB      0.0 MiB           1           .take(10)\n",
            "    54                                             \n",
            "    55    184.6 MiB     -0.0 MiB           1       df3.unpersist()\n",
            "    56                                         \n",
            "    57                                             # termino ejecucion de spark\n",
            "    58    184.6 MiB     -0.1 MiB           1       spark.stop_spark()\n",
            "    59    184.6 MiB      0.0 MiB          13       return [(row.emoji, row.emojiCount) for row in top_10_emojis]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('üôè', 5538),\n",
              " ('üòÇ', 3712),\n",
              " ('üöú', 3017),\n",
              " ('üåæ', 2288),\n",
              " ('üáÆüá≥', 2231),\n",
              " ('ü§£', 1997),\n",
              " ('‚úä', 1776),\n",
              " ('üôèüèª', 1495),\n",
              " ('‚ù§Ô∏è', 1478),\n",
              " ('üëá', 1118)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q2_memory import q2_memory\n",
        "q2_mem_result = q2_memory(f\"{parquet_path}\")\n",
        "q2_mem_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         5400705 function calls (5400628 primitive calls) in 3.264 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 473 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "  1168115    1.282    0.000    1.889    0.000 tokenizer.py:158(tokenize)\n",
            "   133454    0.464    0.000    0.464    0.000 {method 'sub' of 're.Pattern' objects}\n",
            "   133454    0.446    0.000    2.451    0.000 core.py:299(<listcomp>)\n",
            "  1034680    0.187    0.000    0.189    0.000 {built-in method __new__ of type object at 0x00007FF8B5084C60}\n",
            "        1    0.120    0.120    0.121    0.121 pandas_compat.py:755(table_to_dataframe)\n",
            "1035780/1035772    0.117    0.000    0.117    0.000 {built-in method builtins.isinstance}\n",
            "  1034722    0.103    0.000    0.103    0.000 {method 'append' of 'list' objects}\n",
            "   133454    0.098    0.000    2.655    0.000 q2_time.py:6(find_all_emojis)\n",
            "        1    0.081    0.081    0.082    0.082 core.py:1389(read)\n",
            "   133454    0.080    0.000    2.532    0.000 core.py:292(emoji_list)\n",
            "        2    0.059    0.030    0.573    0.286 object_array.py:46(_str_map)\n",
            "        1    0.049    0.049    2.703    2.703 algorithms.py:1667(map_array)\n",
            "   133454    0.035    0.000    0.499    0.000 object_array.py:175(<lambda>)\n",
            "   133454    0.026    0.000    0.026    0.000 tokenizer.py:300(get_search_tree)\n",
            "   133454    0.025    0.000    0.025    0.000 q2_time.py:7(<listcomp>)\n",
            "        3    0.024    0.008    0.024    0.008 missing.py:305(_isna_string_dtype)\n",
            "138722/138665    0.015    0.000    0.015    0.000 {built-in method builtins.len}\n",
            "    48513    0.014    0.000    0.014    0.000 tokenizer.py:29(__init__)\n",
            "        2    0.005    0.003    0.005    0.003 accessor.py:207(_validate)\n",
            "        1    0.004    0.004    0.004    0.004 {built-in method _collections._count_elements}\n",
            "        1    0.003    0.003    3.520    3.520 q2_time.py:9(q2_time)\n",
            "        1    0.003    0.003    0.004    0.004 take.py:120(_take_nd_ndarray)\n",
            "        1    0.003    0.003    0.004    0.004 series.py:4496(explode)\n",
            "        1    0.002    0.002    0.207    0.207 parquet.py:498(read_parquet)\n",
            "      398    0.002    0.000    0.006    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
            "        1    0.002    0.002    0.002    0.002 core.py:1243(__init__)\n",
            "      199    0.002    0.000    0.002    0.000 threading.py:1388(enumerate)\n",
            "      199    0.001    0.000    0.002    0.000 ipkernel.py:790(<setcomp>)\n",
            "     1393    0.001    0.000    0.001    0.000 threading.py:1109(ident)\n",
            "        1    0.001    0.001    0.001    0.001 {method 'tolist' of 'numpy.ndarray' objects}\n",
            "        1    0.001    0.001    0.001    0.001 managers.py:1954(get_rows_with_mask)\n",
            "        5    0.000    0.000    0.001    0.000 base.py:5373(__getitem__)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'repeat' of 'numpy.ndarray' objects}\n",
            "        1    0.000    0.000    0.000    0.000 utils.py:239(maybe_convert_indices)\n",
            "        9    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
            "        6    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
            "        7    0.000    0.000    0.000    0.000 numeric.py:300(full)\n",
            "        3    0.000    0.000    0.005    0.002 frame.py:4062(__getitem__)\n",
            "        8    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method nt.stat}\n",
            "        7    0.000    0.000    0.001    0.000 series.py:389(__init__)\n",
            "        1    0.000    0.000    0.000    0.000 heapq.py:521(nlargest)\n",
            "      797    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
            "        3    0.000    0.000    0.000    0.000 cast.py:1157(maybe_infer_to_datetimelike)\n",
            "       28    0.000    0.000    0.000    0.000 java_gateway.py:1359(<lambda>)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method _operator.gt}\n",
            "      399    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
            "       10    0.000    0.000    0.000    0.000 construction.py:517(sanitize_array)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method _operator.eq}\n",
            "       56    0.000    0.000    0.000    0.000 protocol.py:214(smart_decode)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
            "        2    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
            "       10    0.000    0.000    0.000    0.000 generic.py:6236(__finalize__)\n",
            "        1    0.000    0.000    0.204    0.204 parquet.py:239(read)\n",
            "       12    0.000    0.000    0.000    0.000 generic.py:278(__init__)\n",
            "      227    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
            "       28    0.000    0.000    0.000    0.000 java_gateway.py:643(_garbage_collect_object)\n",
            "       78    0.000    0.000    0.000    0.000 generic.py:37(_check)\n",
            "       21    0.000    0.000    0.000    0.000 generic.py:6301(__setattr__)\n",
            "    23/19    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
            "        3    0.000    0.000    0.000    0.000 base.py:475(__new__)\n",
            "        7    0.000    0.000    0.000    0.000 managers.py:1863(from_array)\n",
            "        2    0.000    0.000    0.000    0.000 accessor.py:255(_wrap_result)\n",
            "       28    0.000    0.000    0.000    0.000 finalizer.py:45(remove_finalizer)\n",
            "       78    0.000    0.000    0.000    0.000 generic.py:42(_instancecheck)\n",
            "        2    0.000    0.000    0.005    0.003 accessor.py:188(__init__)\n",
            "        6    0.000    0.000    0.000    0.000 base.py:649(_simple_new)\n",
            "        3    0.000    0.000    0.000    0.000 common.py:97(is_bool_indexer)\n",
            "        2    0.000    0.000    0.000    0.000 managers.py:1012(iget)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:339(__init__)\n",
            "        1    0.000    0.000    0.004    0.004 __init__.py:649(update)\n",
            "      157    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
            "        2    0.000    0.000    0.000    0.000 sre_compile.py:292(_optimize_charset)\n",
            "        5    0.000    0.000    0.025    0.005 missing.py:184(_isna)\n",
            "       28    0.000    0.000    0.000    0.000 clientserver.py:230(garbage_collect_object)\n",
            "        2    0.000    0.000    0.000    0.000 sre_parse.py:494(_parse)\n",
            "       10    0.000    0.000    0.000    0.000 managers.py:2004(internal_values)\n",
            "       15    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
            "       13    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
            "        9    0.000    0.000    0.000    0.000 config.py:127(_get_single_key)\n",
            "        7    0.000    0.000    0.000    0.000 blocks.py:2716(new_block)\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:124(maybe_convert_platform)\n",
            "       11    0.000    0.000    0.000    0.000 series.py:784(name)\n",
            "       21    0.000    0.000    0.000    0.000 managers.py:1993(dtype)\n",
            "        1    0.000    0.000    0.000    0.000 common.py:664(get_handle)\n",
            "      5/1    0.000    0.000    0.000    0.000 sre_compile.py:87(_compile)\n",
            "        2    0.000    0.000    0.000    0.000 array_ops.py:288(comparison_op)\n",
            "       19    0.000    0.000    0.000    0.000 series.py:734(name)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1122(_add_any_metadata)\n",
            "        9    0.000    0.000    0.000    0.000 common.py:1596(pandas_dtype)\n",
            "       11    0.000    0.000    0.000    0.000 generic.py:6284(__getattr__)\n",
            "        2    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
            "        9    0.000    0.000    0.000    0.000 config.py:635(_get_root)\n",
            "        1    0.000    0.000    0.000    0.000 api.py:39(make_block)\n",
            "       27    0.000    0.000    0.000    0.000 inference.py:334(is_hashable)\n",
            "        2    0.000    0.000    0.000    0.000 fromnumeric.py:89(_wrapreduction_any_all)\n",
            "        7    0.000    0.000    0.000    0.000 generic.py:807(_set_axis)\n",
            "        1    0.000    0.000    0.000    0.000 range.py:1148(take)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('üôè', 5538),\n",
              " ('üòÇ', 3712),\n",
              " ('üöú', 3017),\n",
              " ('üåæ', 2288),\n",
              " ('üáÆüá≥', 2231),\n",
              " ('ü§£', 1997),\n",
              " ('‚úä', 1776),\n",
              " ('üôèüèª', 1495),\n",
              " ('‚ù§Ô∏è', 1478),\n",
              " ('üëá', 1118)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q2_time import q2_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q2_time_result = q2_time(f\"{parquet_path}\")\n",
        "\n",
        "q2_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q2_stats.dump_stats('./stats/q2_time')\n",
        "q2_stats.print_stats(max_print_stat)\n",
        "q2_time_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 3\n",
        "El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(\"LATAM321\", 387), (\"LATAM_CHI\", 129), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">En esta pregunta y como ya ten√≠a reducida la columna \"mentionedUsers\", se me hizo mas f√°cil trabajarla. Como mejora, creo que en el parquet, podr√≠a agregar una nueva columna que cuente de una las veces que se repite el usuario usando \"user.username\", de este forma no tendr√≠a que pasar por el explode, y simplemente podr√≠a sumar este n√∫mero.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: f:\\GitHub\\DataEngineer\\latam_de_challenge\\src\\q3_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     6    244.5 MiB    244.5 MiB           1   @profile\n",
            "     7                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "     8                                             # Inicializacion de Spark\n",
            "     9    244.6 MiB      0.1 MiB           1       spark = SparkClass(\"Q3: Memory\")\n",
            "    10                                             # Carga de datos\n",
            "    11    244.6 MiB      0.0 MiB           1       df = spark.load_parquet(file_path).select(\"mentionUser\").cache()\n",
            "    12                                         \n",
            "    13                                             # hago un explode de los mentionedUsers para abrir el array y luego tomo solo el nombre de usuario\n",
            "    14    244.6 MiB      0.0 MiB           1       df = df.select(sf.explode(\"mentionUser\").alias(\"username\")).select(\"username\")\n",
            "    15                                         \n",
            "    16                                             # obtengo las top 10 fechas con mas tweets\n",
            "    17    244.6 MiB      0.0 MiB           4       top_10_users = df.groupBy(\"username\") \\\n",
            "    18    244.6 MiB      0.0 MiB           1           .agg(sf.count(\"username\").alias(\"mentionCount\")) \\\n",
            "    19    244.6 MiB      0.0 MiB           1           .orderBy(sf.desc(\"mentionCount\")) \\\n",
            "    20    244.6 MiB      0.0 MiB           1           .take(10)\n",
            "    21                                             \n",
            "    22                                             # libero memoria de df\n",
            "    23    244.6 MiB      0.0 MiB           1       df.unpersist()\n",
            "    24                                             \n",
            "    25                                             # termino ejecucion de spark\n",
            "    26    244.5 MiB     -0.1 MiB           1       spark.stop_spark()\n",
            "    27    244.5 MiB      0.0 MiB          13       return [(row.username, row.mentionCount) for row in top_10_users]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('narendramodi', 2415),\n",
              " ('Kisanektamorcha', 1942),\n",
              " ('RakeshTikaitBKU', 1723),\n",
              " ('PMOIndia', 1489),\n",
              " ('RahulGandhi', 1218),\n",
              " ('GretaThunberg', 1105),\n",
              " ('RaviSinghKA', 1062),\n",
              " ('rihanna', 1037),\n",
              " ('UNHumanRights', 989),\n",
              " ('meenaharris', 975)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q3_memory import q3_memory\n",
        "q3_mem_result = q3_memory(f\"{parquet_path}\")\n",
        "q3_mem_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         1731 function calls (1694 primitive calls) in 0.095 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 370 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.031    0.031    0.032    0.032 series.py:4496(explode)\n",
            "        1    0.028    0.028    0.029    0.029 pandas_compat.py:755(table_to_dataframe)\n",
            "        1    0.015    0.015    0.016    0.016 core.py:1389(read)\n",
            "        1    0.006    0.006    0.006    0.006 {built-in method _collections._count_elements}\n",
            "        1    0.003    0.003    0.003    0.003 heapq.py:521(nlargest)\n",
            "        1    0.002    0.002    0.002    0.002 missing.py:305(_isna_string_dtype)\n",
            "        1    0.002    0.002    0.002    0.002 core.py:1243(__init__)\n",
            "        1    0.002    0.002    0.002    0.002 {method 'tolist' of 'numpy.ndarray' objects}\n",
            "        1    0.001    0.001    0.095    0.095 q3_time.py:5(q3_time)\n",
            "        1    0.001    0.001    0.001    0.001 {method 'repeat' of 'numpy.ndarray' objects}\n",
            "        1    0.000    0.000    0.001    0.001 take.py:120(_take_nd_ndarray)\n",
            "        2    0.000    0.000    0.000    0.000 cast.py:1157(maybe_infer_to_datetimelike)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
            "        5    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
            "        3    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
            "        2    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
            "        1    0.000    0.000    0.000    0.000 utils.py:239(maybe_convert_indices)\n",
            "  342/338    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
            "        6    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method nt.stat}\n",
            "        3    0.000    0.000    0.000    0.000 numeric.py:300(full)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method _operator.eq}\n",
            "        3    0.000    0.000    0.000    0.000 base.py:475(__new__)\n",
            "        6    0.000    0.000    0.001    0.000 construction.py:517(sanitize_array)\n",
            "        1    0.000    0.000    0.047    0.047 parquet.py:239(read)\n",
            "        3    0.000    0.000    0.001    0.000 series.py:389(__init__)\n",
            "   107/75    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
            "        3    0.000    0.000    0.002    0.001 frame.py:4062(__getitem__)\n",
            "        5    0.000    0.000    0.000    0.000 base.py:649(_simple_new)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:339(__init__)\n",
            "        7    0.000    0.000    0.000    0.000 generic.py:278(__init__)\n",
            "        5    0.000    0.000    0.000    0.000 generic.py:6236(__finalize__)\n",
            "        1    0.000    0.000    0.000    0.000 astype.py:56(_astype_nansafe)\n",
            "        2    0.000    0.000    0.000    0.000 managers.py:1012(iget)\n",
            "       13    0.000    0.000    0.000    0.000 generic.py:6301(__setattr__)\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:124(maybe_convert_platform)\n",
            "        5    0.000    0.000    0.000    0.000 common.py:1596(pandas_dtype)\n",
            "        1    0.000    0.000    0.047    0.047 parquet.py:498(read_parquet)\n",
            "        1    0.000    0.000    0.001    0.001 pandas_compat.py:886(_deserialize_column_index)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1122(_add_any_metadata)\n",
            "       44    0.000    0.000    0.000    0.000 generic.py:42(_instancecheck)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1056(_reconstruct_columns_from_metadata)\n",
            "       44    0.000    0.000    0.000    0.000 generic.py:37(_check)\n",
            "        1    0.000    0.000    0.000    0.000 common.py:664(get_handle)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:818(_get_extension_dtypes)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:919(_reconstruct_index)\n",
            "       72    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
            "       11    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
            "        1    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
            "        2    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
            "        1    0.000    0.000    0.000    0.000 _optional.py:85(import_optional_dependency)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1085(<listcomp>)\n",
            "        1    0.000    0.000    0.000    0.000 api.py:39(make_block)\n",
            "     10/9    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
            "        5    0.000    0.000    0.000    0.000 config.py:127(_get_single_key)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:888(<dictcomp>)\n",
            "        1    0.000    0.000    0.000    0.000 range.py:1148(take)\n",
            "        1    0.000    0.000    0.001    0.001 base.py:1238(repeat)\n",
            "        3    0.000    0.000    0.000    0.000 managers.py:1863(from_array)\n",
            "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1002(_find_and_load)\n",
            "        2    0.000    0.000    0.002    0.001 missing.py:184(_isna)\n",
            "        7    0.000    0.000    0.000    0.000 base.py:7688(maybe_extract_name)\n",
            "        1    0.000    0.000    0.017    0.017 core.py:1745(read_table)\n",
            "        1    0.000    0.000    0.000    0.000 range.py:137(__new__)\n",
            "       15    0.000    0.000    0.000    0.000 inference.py:334(is_hashable)\n",
            "        1    0.000    0.000    0.000    0.000 base.py:842(_engine)\n",
            "        6    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n",
            "        1    0.000    0.000    0.000    0.000 base.py:1045(astype)\n",
            "        1    0.000    0.000    0.000    0.000 _validators.py:168(validate_args_and_kwargs)\n",
            "       11    0.000    0.000    0.000    0.000 construction.py:481(ensure_wrapped_if_datetimelike)\n",
            "        2    0.000    0.000    0.000    0.000 common.py:231(asarray_tuplesafe)\n",
            "        4    0.000    0.000    0.000    0.000 base.py:5373(__getitem__)\n",
            "        5    0.000    0.000    0.000    0.000 config.py:635(_get_root)\n",
            "       72    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
            "        2    0.000    0.000    0.000    0.000 frame.py:4626(_get_item_cache)\n",
            "        2    0.000    0.000    0.000    0.000 decoder.py:332(decode)\n",
            "        2    0.000    0.000    0.000    0.000 base.py:5323(__contains__)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:520(_cmpkey)\n",
            "        1    0.000    0.000    0.000    0.000 common.py:304(_get_filepath_or_buffer)\n",
            "        5    0.000    0.000    0.000    0.000 config.py:145(_get_option)\n",
            "        4    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:1580(construct_1d_object_array_from_listlike)\n",
            "       18    0.000    0.000    0.000    0.000 range.py:999(__len__)\n",
            "        1    0.000    0.000    0.003    0.003 __init__.py:600(most_common)\n",
            "        1    0.000    0.000    0.001    0.001 frame.py:4130(_getitem_bool_array)\n",
            "        1    0.000    0.000    0.001    0.001 generic.py:4027(take)\n",
            "        1    0.000    0.000    0.000    0.000 base.py:2313(is_unique)\n",
            "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:166(_get_module_lock)\n",
            "        1    0.000    0.000    0.001    0.001 managers.py:623(reindex_indexer)\n",
            "        1    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
            "        7    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
            "        5    0.000    0.000    0.000    0.000 generic.py:6284(__getattr__)\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:551(maybe_promote)\n",
            "        2    0.000    0.000    0.000    0.000 frame.py:3983(_ixs)\n",
            "        3    0.000    0.000    0.000    0.000 managers.py:180(blknos)\n",
            "        9    0.000    0.000    0.000    0.000 managers.py:1993(dtype)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:299(loads)\n",
            "        1    0.000    0.000    0.000    0.000 parquet.py:85(_get_path_or_handle)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('narendramodi', 2415),\n",
              " ('Kisanektamorcha', 1942),\n",
              " ('RakeshTikaitBKU', 1723),\n",
              " ('PMOIndia', 1489),\n",
              " ('RahulGandhi', 1218),\n",
              " ('GretaThunberg', 1105),\n",
              " ('RaviSinghKA', 1062),\n",
              " ('rihanna', 1037),\n",
              " ('UNHumanRights', 989),\n",
              " ('meenaharris', 975)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q3_time import q3_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q3_time_result = q3_time(f\"{parquet_path}\")\n",
        "\n",
        "q3_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q3_stats.dump_stats('./stats/q3_time')\n",
        "q3_stats.print_stats(max_print_stat)\n",
        "q3_time_result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

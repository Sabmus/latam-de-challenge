{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 12px\">En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc.</p>\n",
        "\n",
        "<div hidden>\n",
        "    <!-- Font normal -->\n",
        "    <p style=\"font-size: 16px\">text</p>\n",
        "    <!-- Font pequeño -->\n",
        "    <p style=\"font-size: 15px\">text</p>\n",
        "    <!-- hr interno -->\n",
        "    <hr style=\"height: 1px; color: hsl(221.25 24.24% 18%); background: hsl(221.25 24.24% 18%); font-size: 0; border: 0\">\n",
        "    <!-- hr externo -->\n",
        "    <hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">\n",
        "    <b style=\"color: hsl(163.3 98.98% 58.43%)\">bold</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 align=\"center\">LATAM Data Engineer Challenge</h1>\n",
        "<hr style=\"height: 1px; color: hsl(221.25 24.24% 18%); background: hsl(221.25 24.24% 18%); font-size: 0; border: 0\">\n",
        "<p style=\"font-size: 16px\">Hola! gracias por leer! A continuación paso a detallar los pasos realizados y todo lo pensado para este desafio.</p>\n",
        "\n",
        "<div style=\"background-color: hsl(221.25 24.24% 18%); width: 30%; padding:5px 10px; border-radius: 5px\">\n",
        "<h3>Tech usadas</h3>\n",
        "<ul>\n",
        "    <li>PySpark v3.5.1</li>\n",
        "    <li>Python v3.9.13</li>\n",
        "    <li>Pandas v2.2.2</li>\n",
        "    <li>Java version \"1.8.0_381\"</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "\n",
        "<h2>Proceso mental</h2>\n",
        "<p style=\"font-size: 16px\">Después de revisar la estructura del dataset y revisar la documentación, pensé que lo primero que debía hacer era buscar una manera de unir todos los <b style=\"color: hsl(163.3 98.98% 58.43%)\">quotedTweets</b> ya que finalmente corresponden a otro tweet, y con esto lograr un mayor dataset.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">Siguiendo, y como quería usar PySpark creé una clase que instanciara a <b style=\"color: hsl(163.3 98.98% 58.43%)\">Spark</b> para dejar ahí toda la configuración y cualquier método que me fuera útil. Fui probando distintas configuraciones de <b style=\"color: hsl(163.3 98.98% 58.43%)\">Spark</b> según avanzaba en el desafio, y según errores que iba encontrando.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">Principalmente sufrí con un par de errores del tipo: \"<i>Py4JJavaError: An error occurred while calling {ABC}</i>\". Esto ocurría todas las veces que usé UDFs, busqué varias soluciones pero ninguna me sirvió así que opté por no usar UDFs. Pienso que probablemente tengo algo mal seteado en Spark, y por ello me arrojó error cada vez que quise usarlas.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">Luego de realizar algunas pruebas con el archivo JSON, preferí pasarlo a Parquet para así mejorar el rendimiento de las ejecuciones.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">En cuanto a las respuestas de Tiempo usé <b style=\"color: hsl(163.3 98.98% 58.43%)\">Pandas</b>. Al principio pensé en usar PySpark para todo pero como corría bastante bien en tiempo, pensé que finalmente según se pueda tunear Spark, se puede lograr un buen tiempo de ejecución así como de manejo de memoría. Asi que para variar también mis respuestas me fui por ese camino.</p>\n",
        "\n",
        "<h2>Mejoras</h2>\n",
        "<p style=\"font-size: 16px\">Con respecto a las mejoras se me ocurren varias cosas: </p>\n",
        "<ol>\n",
        "    <li style=\"font-size: 16px\">Leyendo algunos artículos sobre optimización, creo que se podría utilizar Bucketing, Partitioning y Z-Ordering:</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">Partitioning: en el sentido de crear particiones usando alguna columna en particular según se necesite: 'df.write.partitionBy(\"column_name\")'</li>\n",
        "            <li style=\"font-size: 16px\">Bucketing: con esto se puede agrupar data en \"buckets\" del mismo tamaño, lo cual ayuda cuando se usa groupBy, joins y orderBy</li>\n",
        "            <li style=\"font-size: 16px\">Z-Ordering: para ordernar espacialmente la data y juntar data relacionada.</li>\n",
        "        </ul>\n",
        "    <li style=\"font-size: 16px\">Reducir aún más el dataset. Para las 3 preguntas solo se necesitaban 5 columnas: \"id\", \"date\", \"user.username\", \"mentionedUser[].username\" y \"content\":</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">teniendo un dataset más pequeño haría que mejora el rendimiento en el sentido de la carga inicial de la data.</li>\n",
        "        </ul>\n",
        "    <li style=\"font-size: 16px\">Trabajar aún mas la columna \"content\":</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">Para la pregunta 2, como finalmente solo se querían contar emojis lo ideal sería solo dejar los carácteres de emojis. La forma que usé para esto fue quitar casi todo pero me quedaron carácteres de lenguajes de la India.</li>\n",
        "        </ul>\n",
        "    <li style=\"font-size: 16px\">Pensando en Spark, idea contar con mayor cantidad de clústers, según se necesite. Esto aporta escalabilidad según crezcan los datos.</li>\n",
        "    <li style=\"font-size: 16px\">Subir todo a la nube:</li>\n",
        "        <ul>\n",
        "            <li style=\"font-size: 16px\">Subir los archivos .py a un bucket en GCP</li>\n",
        "            <li style=\"font-size: 16px\">Luego con el ID de los archivos en el bucket, usarlos para crear Job en Dataproc</li>\n",
        "            <li style=\"font-size: 16px\">Acá podríamos dejar la data en BigQuery para luego procesarla con alguna herramienta visual</li>\n",
        "            <li style=\"font-size: 16px\">Todo esto se podría orquestar con Airflow y así monitorear las ejecuciones</li>\n",
        "        </ul>\n",
        "</ol>\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"height: 1px; color: hsl(221.25 24.24% 18%); background: hsl(221.25 24.24% 18%); font-size: 0; border: 0\">\n",
        "\n",
        "<p style=\"font-size: 14px\">Referencias</p>\n",
        "<ul>\n",
        "    <li><a href=\"https://medium.com/@dipan.saha/getting-started-with-pyspark-day-1-37e5e6fdc14b\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Instalar PySpark en windows</a></li>\n",
        "    <li><a href=\"https://spark.apache.org/docs/3.0.2/configuration.html\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Docs de configuración de PySpark</a></li>\n",
        "    <li><a href=\"https://medium.com/@shilpajoshi1635/optimizing-query-performance-in-pyspark-with-partitioning-bucketing-and-z-ordering-a5e215e6c272\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Partitioning, Bucketing y Z-ordering</a></li>\n",
        "    <li><a href=\"https://www.freecodecamp.org/news/what-is-google-dataproc/\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Usemos Dataproc!</a></li>\n",
        "    <li><a href=\"https://www.youtube.com/live/R2c6sZYahe0?t=1126s\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">YT sobre pipelines usando PySpark y Airflow con GCP</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Inicialización de variables</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 16px\">Creé la celda siguiente para chequear la instalación de Java.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check de java\n",
        "from subprocess import check_output, STDOUT\n",
        "try:\n",
        "  check_output(\"java -version\", stderr=STDOUT, shell=True).decode('utf-8')\n",
        "except OSError:\n",
        "  # raise KeyboardInterrupt(\"Por favor instale Java JDK para continuar.\")\n",
        "  print(\"Java no está instalado en su PC, para que el programa funcione correctamente, por favor instale Java en su PC.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 16px\">Luego inicializo los nombres de archivos y las rutas, además de preguntar si es que existe el .zip para descomprimirlo en la carpeta <b style=\"color: hsl(163.3 98.98% 58.43%)\">data</b></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cProfile\n",
        "import pstats\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# declaro nombres de archivo y rutas\n",
        "json_file = \"farmers-protest-tweets-2021-2-4.json\"\n",
        "parquet_file = \"tweets.parquet\"\n",
        "data_folder_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data'))\n",
        "json_path = f\"{data_folder_path}\\\\{json_file}\"\n",
        "parquet_path = f\"{data_folder_path}\\\\{parquet_file}\"\n",
        "\n",
        "# max lineas a imprimir en stats\n",
        "max_print_stat = 100\n",
        "\n",
        "# checkea si existe la carpeta data, sino, la crea\n",
        "if not os.path.exists(data_folder_path):\n",
        "    os.makedirs(data_folder_path)\n",
        "\n",
        "# checkea si existe el archivo tweets.json.zip, si no, checkea si existe el archivo json, si no, lanza excepción\n",
        "if not os.path.exists(f\"{data_folder_path}\\\\tweets.json.zip\"):\n",
        "    if not os.path.exists(f\"{data_folder_path}\\\\{json_file}\"):\n",
        "        raise FileNotFoundError(\"Por favor descargue el archivo 'tweets.json.zip' y colóquelo en la carpeta 'data'\")\n",
        "\n",
        "# si no existe el archivo json, descomprime el archivo zip\n",
        "if not os.path.exists(f\"{data_folder_path}\\\\{json_file}\"):\n",
        "    with ZipFile(f\"{data_folder_path}\\\\tweets.json.zip\", 'r') as zObject: \n",
        "        zObject.extractall(path=f\"{data_folder_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 16px\">Luego pregunta si es que existe el archivo parquet, sino, lo crea.</p>\n",
        "\n",
        "<p style=\"font-size: 16px\">La función <b style=\"color: hsl(163.3 98.98% 58.43%)\">json_to_parquet</b> toma el json principal (el de 389mb) y lo \"desnestea\". Esto va a buscar si existe algún elemento <b style=\"color: hsl(163.3 98.98% 58.43%)\">quotedTweet</b>, y si existe une sus datos con el dataframe principal, esto para crear un gran dataset con todos los tweets, luego de ellos lo pasa a formato <b style=\"color: hsl(163.3 98.98% 58.43%)\">parquet</b>.</p>\n",
        "\n",
        "```python\n",
        "def json_to_parquet(ruta_del_json: str, ruta_del_parquet: str) -> None\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from helpers.helperFuncs import json_to_parquet\n",
        "# si no encuentra el archivo parquet, lo crea\n",
        "if not os.path.exists(f\"{parquet_path}\"):\n",
        "    json_to_parquet(json_path, parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 1\n",
        "Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "```\n",
        "```python\n",
        "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(datetime.date(1999, 11, 15), \"LATAM321\"), (datetime.date(1999, 7, 15), \"LATAM_CHI\"), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">Creo que las mejoras que se pueden hacer acá es no utilizar \"limit\", y trabajar sobre las listas resultantes usando \"take\". Leí que limit se hace en dos pasos, uno local y otro global, este último genera un shuffle lo cual es costoso en dataset grandes. <a href=\"https://towardsdatascience.com/stop-using-the-limit-clause-wrong-with-spark-646e328774f5\" style=\"color: hsl(163.3 98.98% 58.43%); font-size: 14px\" target=\"_blank\">Referencia</a></p>\n",
        "\n",
        "```python\n",
        " # obtengo las top 10 fechas con mas tweets \n",
        "top_10_dates = df.groupBy(df[\"date_only\"].alias(\"date\")) \\\n",
        "    .agg(sf.count(\"id\").alias(\"tweetCount\")) \\\n",
        "    .orderBy(sf.desc(\"tweetCount\")) \\\n",
        "    .limit(10).cache()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: f:\\GitHub\\DataEngineer\\latam_de_challenge\\src\\q1_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     8     73.2 MiB     73.2 MiB           1   @profile\n",
            "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
            "    10                                             # Inicializacion de Spark\n",
            "    11     75.5 MiB      2.3 MiB           1       spark = SparkClass(\"Q1: Memory\")\n",
            "    12                                             # Carga de datos\n",
            "    13     75.5 MiB      0.0 MiB           1       df = spark.load_parquet(file_path).select(\"id\", \"username\", \"date\")\n",
            "    14     75.6 MiB      0.1 MiB           3       df = df.withColumn(\"date_only\", sf.col(\"date\").cast(DateType())) \\\n",
            "    15     75.5 MiB      0.0 MiB           1           .repartition(4, \"date_only\") \\\n",
            "    16     75.5 MiB      0.0 MiB           1           .drop(\"date\").cache()\n",
            "    17                                         \n",
            "    18                                             # obtengo las top 10 fechas con mas tweets \n",
            "    19     75.6 MiB      0.0 MiB           4       top_10_dates = df.groupBy(df[\"date_only\"].alias(\"date\")) \\\n",
            "    20     75.6 MiB      0.0 MiB           1           .agg(sf.count(\"id\").alias(\"tweetCount\")) \\\n",
            "    21     75.6 MiB      0.0 MiB           1           .orderBy(sf.desc(\"tweetCount\")) \\\n",
            "    22     75.6 MiB      0.0 MiB           1           .limit(10).cache()\n",
            "    23                                             \n",
            "    24                                             # filtro el df principal con las top 10 fechas para luego agrupar según cantidad de tweets por usuario\n",
            "    25     76.2 MiB      0.6 MiB          13       filtro = df.date_only.isin([row.date for row in top_10_dates.take(10)])\n",
            "    26     76.3 MiB      0.1 MiB           5       top_user_by_date = df.filter(filtro) \\\n",
            "    27     76.2 MiB      0.0 MiB           1           .groupBy(df[\"date_only\"].alias(\"date\"), \"username\") \\\n",
            "    28     76.2 MiB      0.0 MiB           2           .agg(sf.count(\"id\").alias(\"tweetCount\")).orderBy(sf.desc(\"tweetCount\")) \\\n",
            "    29     76.2 MiB      0.0 MiB           1           .limit(10).cache()\n",
            "    30                                             \n",
            "    31                                             # libero memoria de df\n",
            "    32     76.3 MiB      0.0 MiB           1       df.unpersist()\n",
            "    33                                             \n",
            "    34                                             # hago un join con ambos df para obtener el resultado final\n",
            "    35     76.3 MiB      0.0 MiB           5       result = top_10_dates.alias(\"top_10_dates\") \\\n",
            "    36     76.3 MiB      0.0 MiB           1               .join(top_user_by_date.alias(\"top_user_by_date\"),top_10_dates.date == top_user_by_date.date, \"inner\") \\\n",
            "    37     76.3 MiB      0.0 MiB           1               .select(\"top_10_dates.date\", \"top_user_by_date.username\") \\\n",
            "    38     76.3 MiB      0.0 MiB           1               .orderBy(sf.desc(\"top_10_dates.tweetCount\")) \\\n",
            "    39     76.3 MiB      0.0 MiB           1               .take(10)\n",
            "    40                                             \n",
            "    41                                             # libero memoria de top_10_dates y top_user_by_date\n",
            "    42     76.3 MiB      0.0 MiB           1       top_10_dates.unpersist()\n",
            "    43     76.3 MiB      0.0 MiB           1       top_user_by_date.unpersist()\n",
            "    44                                         \n",
            "    45                                             # termino ejecucion de spark\n",
            "    46     76.2 MiB     -0.1 MiB           1       spark.stop_spark()\n",
            "    47     76.2 MiB      0.0 MiB          13       return [(row.date, row.username) for row in result]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
              " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
              " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
              " (datetime.date(2021, 2, 16), 'jot__b'),\n",
              " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
              " (datetime.date(2021, 2, 18), 'rebelpacifist'),\n",
              " (datetime.date(2021, 2, 15), 'jot__b'),\n",
              " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
              " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
              " (datetime.date(2021, 2, 19), 'KaurDosanjh1979')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q1_memory import q1_memory\n",
        "q1_mem_result = q1_memory(f\"{parquet_path}\")\n",
        "q1_mem_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">Acá (y en realidad lo mismo para el código de memoria) creo que en vez de crear una nueva columna con la fecha, se podría dejar de antemano en el parquet, para así ahorrar la conversión.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         937513 function calls (936682 primitive calls) in 1.393 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 1132 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "    50520    0.358    0.000    0.785    0.000 multi.py:2982(get_loc)\n",
            "        1    0.247    0.247    0.247    0.247 datetimes.py:456(_array_strptime_with_fallback)\n",
            "    50549    0.134    0.000    0.134    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "   101048    0.096    0.000    0.107    0.000 base.py:3777(get_loc)\n",
            "    50521    0.085    0.000    0.219    0.000 multi.py:136(_codes_to_ints)\n",
            "        1    0.069    0.069    0.908    0.908 multi.py:2355(drop)\n",
            "        1    0.059    0.059    0.060    0.060 pandas_compat.py:755(table_to_dataframe)\n",
            "        1    0.034    0.034    0.036    0.036 multi.py:758(_values)\n",
            "        1    0.029    0.029    0.029    0.029 datetimes.py:1436(date)\n",
            "        3    0.029    0.010    0.030    0.010 algorithms.py:548(factorize_array)\n",
            "    50520    0.027    0.000    0.056    0.000 multi.py:2738(_check_indexing_error)\n",
            "   101058    0.027    0.000    0.036    0.000 multi.py:1046(nlevels)\n",
            "       12    0.019    0.002    0.019    0.002 {method 'argsort' of 'numpy.ndarray' objects}\n",
            "    50606    0.016    0.000    0.030    0.000 inference.py:334(is_hashable)\n",
            "        1    0.016    0.016    0.016    0.016 core.py:1389(read)\n",
            "153638/153378    0.015    0.000    0.015    0.000 {built-in method builtins.len}\n",
            "    50630    0.013    0.000    0.013    0.000 {built-in method builtins.hash}\n",
            "104392/104363    0.012    0.000    0.012    0.000 {built-in method builtins.isinstance}\n",
            "   101048    0.011    0.000    0.011    0.000 base.py:6672(_maybe_cast_indexer)\n",
            "       13    0.008    0.001    0.008    0.001 {built-in method _imp.create_dynamic}\n",
            "    52034    0.007    0.000    0.007    0.000 {method 'append' of 'list' objects}\n",
            "       22    0.005    0.000    0.007    0.000 take.py:120(_take_nd_ndarray)\n",
            "  108/105    0.005    0.000    0.005    0.000 {built-in method numpy.asarray}\n",
            "        1    0.005    0.005    0.009    0.009 sorting.py:687(compress_group_index)\n",
            "        1    0.005    0.005    0.005    0.005 algorithms.py:457(isin)\n",
            "     13/8    0.004    0.000    0.013    0.002 {built-in method _imp.exec_dynamic}\n",
            "       88    0.004    0.000    0.004    0.000 {built-in method nt.stat}\n",
            "        1    0.004    0.004    1.393    1.393 q1_time.py:5(q1_time)\n",
            "        6    0.002    0.000    0.002    0.000 missing.py:564(_array_equivalent_object)\n",
            "        2    0.002    0.001    0.065    0.032 ops.py:700(size)\n",
            "        8    0.002    0.000    0.002    0.000 {built-in method io.open_code}\n",
            "        8    0.002    0.000    0.003    0.000 base.py:2313(is_unique)\n",
            "        1    0.002    0.002    0.015    0.015 core.py:1243(__init__)\n",
            "        1    0.002    0.002    0.002    0.002 sorting.py:246(_decons_group_index)\n",
            "       10    0.002    0.000    0.002    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
            "       69    0.002    0.000    0.002    0.000 {built-in method numpy.empty}\n",
            "        2    0.001    0.001    0.919    0.459 selectn.py:55(nlargest)\n",
            "       33    0.001    0.000    0.001    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
            "        3    0.001    0.000    0.001    0.000 algorithms.py:280(_check_object_for_strings)\n",
            "      210    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap_external>:91(_path_join)\n",
            "        1    0.001    0.001    0.001    0.001 multi.py:1193(_engine)\n",
            "        1    0.001    0.001    0.001    0.001 sorting.py:122(get_group_index)\n",
            "        8    0.001    0.000    0.001    0.000 {built-in method marshal.loads}\n",
            "        1    0.001    0.001    0.001    0.001 {method 'put' of 'numpy.ndarray' objects}\n",
            "        3    0.001    0.000    0.050    0.017 algorithms.py:610(factorize)\n",
            "    44/11    0.001    0.000    0.002    0.000 sre_parse.py:494(_parse)\n",
            "        9    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
            "        3    0.001    0.000    0.913    0.304 generic.py:4796(_drop_axis)\n",
            "        1    0.000    0.000    0.004    0.004 datetimes.py:126(_guess_datetime_format_for_array)\n",
            "        3    0.000    0.000    0.001    0.000 utils.py:239(maybe_convert_indices)\n",
            "       15    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
            "     66/6    0.000    0.000    0.001    0.000 sre_compile.py:87(_compile)\n",
            "       52    0.000    0.000    0.006    0.000 <frozen importlib._bootstrap_external>:1514(find_spec)\n",
            "        4    0.000    0.000    0.005    0.001 _function_base_impl.py:5335(delete)\n",
            "        8    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
            "        2    0.000    0.000    0.061    0.030 ops.py:758(_get_compressed_codes)\n",
            "        2    0.000    0.000    0.917    0.459 selectn.py:90(compute)\n",
            "       19    0.000    0.000    0.001    0.000 cast.py:1157(maybe_infer_to_datetimelike)\n",
            "        3    0.000    0.000    0.019    0.006 algorithms.py:1452(safe_sort)\n",
            "   147/19    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
            "      156    0.000    0.000    0.001    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
            "       62    0.000    0.000    0.000    0.000 base.py:649(_simple_new)\n",
            "        1    0.000    0.000    0.000    0.000 datetimelike.py:2353(copy)\n",
            "       18    0.000    0.000    0.002    0.000 base.py:475(__new__)\n",
            "        7    0.000    0.000    0.036    0.005 base.py:836(__iter__)\n",
            "       12    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
            "      871    0.000    0.000    0.000    0.000 sre_parse.py:234(__next)\n",
            "       27    0.000    0.000    0.001    0.000 construction.py:517(sanitize_array)\n",
            "      766    0.000    0.000    0.000    0.000 sre_parse.py:255(get)\n",
            "     26/5    0.000    0.000    0.026    0.005 <frozen importlib._bootstrap>:1002(_find_and_load)\n",
            "        4    0.000    0.000    0.000    0.000 {built-in method builtins.eval}\n",
            "        4    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
            "  640/636    0.000    0.000    0.029    0.000 {built-in method builtins.getattr}\n",
            "        4    0.000    0.000    0.000    0.000 __init__.py:345(namedtuple)\n",
            "       36    0.000    0.000    0.000    0.000 generic.py:6236(__finalize__)\n",
            "       25    0.000    0.000    0.007    0.000 <frozen importlib._bootstrap>:901(_find_spec)\n",
            "       48    0.000    0.000    0.000    0.000 sre_compile.py:292(_optimize_charset)\n",
            "        1    0.000    0.000    0.004    0.004 sorting.py:718(_reorder_by_uniques)\n",
            "       78    0.000    0.000    0.000    0.000 threading.py:1388(enumerate)\n",
            "       20    0.000    0.000    0.000    0.000 cast.py:551(maybe_promote)\n",
            "      462    0.000    0.000    0.000    0.000 sre_parse.py:165(__getitem__)\n",
            "       24    0.000    0.000    0.000    0.000 base.py:5323(__contains__)\n",
            "       34    0.000    0.000    0.000    0.000 numeric.py:300(full)\n",
            "       43    0.000    0.000    0.000    0.000 generic.py:278(__init__)\n",
            "       78    0.000    0.000    0.000    0.000 ipkernel.py:790(<setcomp>)\n",
            "       75    0.000    0.000    0.000    0.000 generic.py:6301(__setattr__)\n",
            "        8    0.000    0.000    0.002    0.000 series.py:389(__init__)\n",
            "      317    0.000    0.000    0.000    0.000 generic.py:42(_instancecheck)\n",
            "      712    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
            "        2    0.000    0.000    0.061    0.030 ops.py:743(group_info)\n",
            "    75/17    0.000    0.000    0.000    0.000 sre_parse.py:175(getwidth)\n",
            "      196    0.000    0.000    0.004    0.000 re.py:289(_compile)\n",
            "       13    0.000    0.000    0.000    0.000 base.py:842(_engine)\n",
            "      317    0.000    0.000    0.000    0.000 generic.py:37(_check)\n",
            "       25    0.000    0.000    0.006    0.000 <frozen importlib._bootstrap_external>:1383(_get_spec)\n",
            "      210    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:114(<listcomp>)\n",
            "      496    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
            "      546    0.000    0.000    0.000    0.000 threading.py:1109(ident)\n",
            "     25/4    0.000    0.000    0.026    0.006 <frozen importlib._bootstrap>:967(_find_and_load_unlocked)\n",
            "        5    0.000    0.000    0.001    0.000 managers.py:708(_slice_take_blocks_ax0)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
              " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
              " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
              " (datetime.date(2021, 2, 16), 'jot__b'),\n",
              " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
              " (datetime.date(2021, 2, 18), 'rebelpacifist'),\n",
              " (datetime.date(2021, 2, 15), 'jot__b'),\n",
              " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
              " (datetime.date(2021, 2, 19), 'Preetm91'),\n",
              " (datetime.date(2021, 2, 19), 'KaurDosanjh1979')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q1_time import q1_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q1_time_result = q1_time(f\"{parquet_path}\")\n",
        "\n",
        "q1_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q1_stats.dump_stats('./stats/q1_time')\n",
        "q1_stats.print_stats(max_print_stat)\n",
        "q1_time_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 2\n",
        "Los top 10 emojis más usados con su respectivo conteo. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(\"✈️\", 6856), (\"❤️\", 5876), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">Como comenté antes, creo que se puede mejorar el patrón de búsqueda de emojis, y el de reducción de la columna \"content\". Para esta pregunta usé la siguiente expresión regular:</p>\n",
        "\n",
        "```shell\n",
        "[\\u0900-\\u097F]+|[\\x00-\\x7F]+\n",
        "```\n",
        "<p style=\"font-size: 16px\">La cual se encarga de quitar casi todo dejandolo solo emojis, pero si bien funciona bien, creo que se puede mejorar para quitar aún mas carácteres y dejar solo emojis.</p>\n",
        "\n",
        "<p style=\"font-size: 14px\"><i>Nota aparte: probé este código usando el patrón completo de emojis sacado de la librería \"emojis\" y me dió el mismo resultado sobre la columna completa sin limpiar.</i></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: f:\\GitHub\\DataEngineer\\latam_de_challenge\\src\\q2_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "    11    184.1 MiB    184.1 MiB           1   @profile\n",
            "    12                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "    13                                             # Inicializacion de Spark\n",
            "    14    184.2 MiB      0.2 MiB           1       spark = SparkClass(\"Q2: Memory\")\n",
            "    15                                             # Carga de datos\n",
            "    16    184.2 MiB      0.0 MiB           1       df = spark.load_parquet(file_path).select(\"content\").where(sf.col(\"content\").isNotNull()).cache()\n",
            "    17                                             \n",
            "    18                                             # creo patron de emojis\n",
            "    19                                             # https://carpedm20.github.io/emoji/docs/index.html#regular-expression\n",
            "    20    184.2 MiB      0.0 MiB           1       emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True) \n",
            "    21    184.8 MiB      0.4 MiB       10071       pattern = '(' + '|'.join(re.escape(u) for u in emojis) + ')'\n",
            "    22                                         \n",
            "    23                                             # quise hacerlo con rdd pero no pude convertirlo a dataframe\n",
            "    24                                             # 'PipelinedRDD' object has no attribute 'toDF' in PySpark\n",
            "    25                                             # https://stackoverflow.com/questions/32788387/pipelinedrdd-object-has-no-attribute-todf-in-pyspark\n",
            "    26                                             #rdd = df.rdd.map(lambda x: (find_all_emojis(x.content))).toDF([\"content\"])\n",
            "    27                                         \n",
            "    28                                             # patrón para eliminar casi todo lo que no sea emoji\n",
            "    29                                             # con esto la idea es reducir el tamaño de \"content\", que tenga menos \n",
            "    30                                             # caracteres a procesar\n",
            "    31    184.6 MiB     -0.2 MiB           1       short_pattern = '([\\u0900-\\u097F]+|[\\x00-\\x7F]+)'\n",
            "    32    184.6 MiB      0.0 MiB           2       content = df.select(sf.regexp_replace(sf.col('content'), fr'{short_pattern}', '').alias(\"content\")) \\\n",
            "    33    184.6 MiB      0.0 MiB           1           .filter(sf.col(\"content\") != \"\").cache()\n",
            "    34                                             \n",
            "    35    184.6 MiB      0.0 MiB           1       df.unpersist()\n",
            "    36                                             \n",
            "    37                                             # extraigo emojis usando expresión regular y el patrón creado\n",
            "    38    184.6 MiB      0.0 MiB           3       df2 = content.withColumn(\"emojis\", sf.expr(f\"regexp_extract_all(content, r'{pattern}')\")) \\\n",
            "    39    184.6 MiB      0.0 MiB           1           .where(sf.size(sf.col(\"emojis\")) > 0) \\\n",
            "    40    184.6 MiB      0.0 MiB           1           .drop(\"content\") \\\n",
            "    41                                                 .cache()\n",
            "    42                                             \n",
            "    43    184.6 MiB      0.0 MiB           1       content.unpersist()\n",
            "    44                                             \n",
            "    45                                             # uso explode para \"abrir\" cada lista de emojis\n",
            "    46    184.6 MiB      0.0 MiB           1       df3 = df2.select(sf.explode(\"emojis\").alias(\"emoji\")).cache()\n",
            "    47                                         \n",
            "    48    184.6 MiB      0.0 MiB           1       df2.unpersist()\n",
            "    49                                         \n",
            "    50    184.6 MiB     -0.0 MiB           4       top_10_emojis = df3.groupBy(\"emoji\") \\\n",
            "    51    184.6 MiB      0.0 MiB           1           .agg(sf.count(\"emoji\").alias(\"emojiCount\")) \\\n",
            "    52    184.6 MiB      0.0 MiB           1           .orderBy(sf.desc(\"emojiCount\")) \\\n",
            "    53    184.6 MiB      0.0 MiB           1           .take(10)\n",
            "    54                                             \n",
            "    55    184.6 MiB     -0.0 MiB           1       df3.unpersist()\n",
            "    56                                         \n",
            "    57                                             # termino ejecucion de spark\n",
            "    58    184.6 MiB     -0.1 MiB           1       spark.stop_spark()\n",
            "    59    184.6 MiB      0.0 MiB          13       return [(row.emoji, row.emojiCount) for row in top_10_emojis]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('🙏', 5538),\n",
              " ('😂', 3712),\n",
              " ('🚜', 3017),\n",
              " ('🌾', 2288),\n",
              " ('🇮🇳', 2231),\n",
              " ('🤣', 1997),\n",
              " ('✊', 1776),\n",
              " ('🙏🏻', 1495),\n",
              " ('❤️', 1478),\n",
              " ('👇', 1118)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q2_memory import q2_memory\n",
        "q2_mem_result = q2_memory(f\"{parquet_path}\")\n",
        "q2_mem_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         5400705 function calls (5400628 primitive calls) in 3.264 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 473 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "  1168115    1.282    0.000    1.889    0.000 tokenizer.py:158(tokenize)\n",
            "   133454    0.464    0.000    0.464    0.000 {method 'sub' of 're.Pattern' objects}\n",
            "   133454    0.446    0.000    2.451    0.000 core.py:299(<listcomp>)\n",
            "  1034680    0.187    0.000    0.189    0.000 {built-in method __new__ of type object at 0x00007FF8B5084C60}\n",
            "        1    0.120    0.120    0.121    0.121 pandas_compat.py:755(table_to_dataframe)\n",
            "1035780/1035772    0.117    0.000    0.117    0.000 {built-in method builtins.isinstance}\n",
            "  1034722    0.103    0.000    0.103    0.000 {method 'append' of 'list' objects}\n",
            "   133454    0.098    0.000    2.655    0.000 q2_time.py:6(find_all_emojis)\n",
            "        1    0.081    0.081    0.082    0.082 core.py:1389(read)\n",
            "   133454    0.080    0.000    2.532    0.000 core.py:292(emoji_list)\n",
            "        2    0.059    0.030    0.573    0.286 object_array.py:46(_str_map)\n",
            "        1    0.049    0.049    2.703    2.703 algorithms.py:1667(map_array)\n",
            "   133454    0.035    0.000    0.499    0.000 object_array.py:175(<lambda>)\n",
            "   133454    0.026    0.000    0.026    0.000 tokenizer.py:300(get_search_tree)\n",
            "   133454    0.025    0.000    0.025    0.000 q2_time.py:7(<listcomp>)\n",
            "        3    0.024    0.008    0.024    0.008 missing.py:305(_isna_string_dtype)\n",
            "138722/138665    0.015    0.000    0.015    0.000 {built-in method builtins.len}\n",
            "    48513    0.014    0.000    0.014    0.000 tokenizer.py:29(__init__)\n",
            "        2    0.005    0.003    0.005    0.003 accessor.py:207(_validate)\n",
            "        1    0.004    0.004    0.004    0.004 {built-in method _collections._count_elements}\n",
            "        1    0.003    0.003    3.520    3.520 q2_time.py:9(q2_time)\n",
            "        1    0.003    0.003    0.004    0.004 take.py:120(_take_nd_ndarray)\n",
            "        1    0.003    0.003    0.004    0.004 series.py:4496(explode)\n",
            "        1    0.002    0.002    0.207    0.207 parquet.py:498(read_parquet)\n",
            "      398    0.002    0.000    0.006    0.000 ipkernel.py:775(_clean_thread_parent_frames)\n",
            "        1    0.002    0.002    0.002    0.002 core.py:1243(__init__)\n",
            "      199    0.002    0.000    0.002    0.000 threading.py:1388(enumerate)\n",
            "      199    0.001    0.000    0.002    0.000 ipkernel.py:790(<setcomp>)\n",
            "     1393    0.001    0.000    0.001    0.000 threading.py:1109(ident)\n",
            "        1    0.001    0.001    0.001    0.001 {method 'tolist' of 'numpy.ndarray' objects}\n",
            "        1    0.001    0.001    0.001    0.001 managers.py:1954(get_rows_with_mask)\n",
            "        5    0.000    0.000    0.001    0.000 base.py:5373(__getitem__)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'repeat' of 'numpy.ndarray' objects}\n",
            "        1    0.000    0.000    0.000    0.000 utils.py:239(maybe_convert_indices)\n",
            "        9    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
            "        6    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
            "        7    0.000    0.000    0.000    0.000 numeric.py:300(full)\n",
            "        3    0.000    0.000    0.005    0.002 frame.py:4062(__getitem__)\n",
            "        8    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method nt.stat}\n",
            "        7    0.000    0.000    0.001    0.000 series.py:389(__init__)\n",
            "        1    0.000    0.000    0.000    0.000 heapq.py:521(nlargest)\n",
            "      797    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
            "        3    0.000    0.000    0.000    0.000 cast.py:1157(maybe_infer_to_datetimelike)\n",
            "       28    0.000    0.000    0.000    0.000 java_gateway.py:1359(<lambda>)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method _operator.gt}\n",
            "      399    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
            "       10    0.000    0.000    0.000    0.000 construction.py:517(sanitize_array)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method _operator.eq}\n",
            "       56    0.000    0.000    0.000    0.000 protocol.py:214(smart_decode)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
            "        2    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
            "       10    0.000    0.000    0.000    0.000 generic.py:6236(__finalize__)\n",
            "        1    0.000    0.000    0.204    0.204 parquet.py:239(read)\n",
            "       12    0.000    0.000    0.000    0.000 generic.py:278(__init__)\n",
            "      227    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
            "       28    0.000    0.000    0.000    0.000 java_gateway.py:643(_garbage_collect_object)\n",
            "       78    0.000    0.000    0.000    0.000 generic.py:37(_check)\n",
            "       21    0.000    0.000    0.000    0.000 generic.py:6301(__setattr__)\n",
            "    23/19    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
            "        3    0.000    0.000    0.000    0.000 base.py:475(__new__)\n",
            "        7    0.000    0.000    0.000    0.000 managers.py:1863(from_array)\n",
            "        2    0.000    0.000    0.000    0.000 accessor.py:255(_wrap_result)\n",
            "       28    0.000    0.000    0.000    0.000 finalizer.py:45(remove_finalizer)\n",
            "       78    0.000    0.000    0.000    0.000 generic.py:42(_instancecheck)\n",
            "        2    0.000    0.000    0.005    0.003 accessor.py:188(__init__)\n",
            "        6    0.000    0.000    0.000    0.000 base.py:649(_simple_new)\n",
            "        3    0.000    0.000    0.000    0.000 common.py:97(is_bool_indexer)\n",
            "        2    0.000    0.000    0.000    0.000 managers.py:1012(iget)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:339(__init__)\n",
            "        1    0.000    0.000    0.004    0.004 __init__.py:649(update)\n",
            "      157    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
            "        2    0.000    0.000    0.000    0.000 sre_compile.py:292(_optimize_charset)\n",
            "        5    0.000    0.000    0.025    0.005 missing.py:184(_isna)\n",
            "       28    0.000    0.000    0.000    0.000 clientserver.py:230(garbage_collect_object)\n",
            "        2    0.000    0.000    0.000    0.000 sre_parse.py:494(_parse)\n",
            "       10    0.000    0.000    0.000    0.000 managers.py:2004(internal_values)\n",
            "       15    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
            "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
            "       13    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
            "        9    0.000    0.000    0.000    0.000 config.py:127(_get_single_key)\n",
            "        7    0.000    0.000    0.000    0.000 blocks.py:2716(new_block)\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:124(maybe_convert_platform)\n",
            "       11    0.000    0.000    0.000    0.000 series.py:784(name)\n",
            "       21    0.000    0.000    0.000    0.000 managers.py:1993(dtype)\n",
            "        1    0.000    0.000    0.000    0.000 common.py:664(get_handle)\n",
            "      5/1    0.000    0.000    0.000    0.000 sre_compile.py:87(_compile)\n",
            "        2    0.000    0.000    0.000    0.000 array_ops.py:288(comparison_op)\n",
            "       19    0.000    0.000    0.000    0.000 series.py:734(name)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1122(_add_any_metadata)\n",
            "        9    0.000    0.000    0.000    0.000 common.py:1596(pandas_dtype)\n",
            "       11    0.000    0.000    0.000    0.000 generic.py:6284(__getattr__)\n",
            "        2    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
            "        9    0.000    0.000    0.000    0.000 config.py:635(_get_root)\n",
            "        1    0.000    0.000    0.000    0.000 api.py:39(make_block)\n",
            "       27    0.000    0.000    0.000    0.000 inference.py:334(is_hashable)\n",
            "        2    0.000    0.000    0.000    0.000 fromnumeric.py:89(_wrapreduction_any_all)\n",
            "        7    0.000    0.000    0.000    0.000 generic.py:807(_set_axis)\n",
            "        1    0.000    0.000    0.000    0.000 range.py:1148(take)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('🙏', 5538),\n",
              " ('😂', 3712),\n",
              " ('🚜', 3017),\n",
              " ('🌾', 2288),\n",
              " ('🇮🇳', 2231),\n",
              " ('🤣', 1997),\n",
              " ('✊', 1776),\n",
              " ('🙏🏻', 1495),\n",
              " ('❤️', 1478),\n",
              " ('👇', 1118)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q2_time import q2_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q2_time_result = q2_time(f\"{parquet_path}\")\n",
        "\n",
        "q2_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q2_stats.dump_stats('./stats/q2_time')\n",
        "q2_stats.print_stats(max_print_stat)\n",
        "q2_time_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"height: 1px; color: hsl(163.3 98.98% 58.43%); background: hsl(163.3 98.98% 58.43%); font-size: 0; border: 0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pregunta 3\n",
        "El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos. Debe incluir las siguientes funciones:\n",
        "```python\n",
        "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
        "```\n",
        "```python\n",
        "Returns: \n",
        "[(\"LATAM321\", 387), (\"LATAM_CHI\", 129), ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Notas:</h4>\n",
        "\n",
        "<p style=\"font-size: 16px\">En esta pregunta y como ya tenía reducida la columna \"mentionedUsers\", se me hizo mas fácil trabajarla. Como mejora, creo que en el parquet, podría agregar una nueva columna que cuente de una las veces que se repite el usuario usando \"user.username\", de este forma no tendría que pasar por el explode, y simplemente podría sumar este número.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filename: f:\\GitHub\\DataEngineer\\latam_de_challenge\\src\\q3_memory.py\n",
            "\n",
            "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
            "=============================================================\n",
            "     6    244.5 MiB    244.5 MiB           1   @profile\n",
            "     7                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
            "     8                                             # Inicializacion de Spark\n",
            "     9    244.6 MiB      0.1 MiB           1       spark = SparkClass(\"Q3: Memory\")\n",
            "    10                                             # Carga de datos\n",
            "    11    244.6 MiB      0.0 MiB           1       df = spark.load_parquet(file_path).select(\"mentionUser\").cache()\n",
            "    12                                         \n",
            "    13                                             # hago un explode de los mentionedUsers para abrir el array y luego tomo solo el nombre de usuario\n",
            "    14    244.6 MiB      0.0 MiB           1       df = df.select(sf.explode(\"mentionUser\").alias(\"username\")).select(\"username\")\n",
            "    15                                         \n",
            "    16                                             # obtengo las top 10 fechas con mas tweets\n",
            "    17    244.6 MiB      0.0 MiB           4       top_10_users = df.groupBy(\"username\") \\\n",
            "    18    244.6 MiB      0.0 MiB           1           .agg(sf.count(\"username\").alias(\"mentionCount\")) \\\n",
            "    19    244.6 MiB      0.0 MiB           1           .orderBy(sf.desc(\"mentionCount\")) \\\n",
            "    20    244.6 MiB      0.0 MiB           1           .take(10)\n",
            "    21                                             \n",
            "    22                                             # libero memoria de df\n",
            "    23    244.6 MiB      0.0 MiB           1       df.unpersist()\n",
            "    24                                             \n",
            "    25                                             # termino ejecucion de spark\n",
            "    26    244.5 MiB     -0.1 MiB           1       spark.stop_spark()\n",
            "    27    244.5 MiB      0.0 MiB          13       return [(row.username, row.mentionCount) for row in top_10_users]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('narendramodi', 2415),\n",
              " ('Kisanektamorcha', 1942),\n",
              " ('RakeshTikaitBKU', 1723),\n",
              " ('PMOIndia', 1489),\n",
              " ('RahulGandhi', 1218),\n",
              " ('GretaThunberg', 1105),\n",
              " ('RaviSinghKA', 1062),\n",
              " ('rihanna', 1037),\n",
              " ('UNHumanRights', 989),\n",
              " ('meenaharris', 975)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q3_memory import q3_memory\n",
        "q3_mem_result = q3_memory(f\"{parquet_path}\")\n",
        "q3_mem_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         1731 function calls (1694 primitive calls) in 0.095 seconds\n",
            "\n",
            "   Ordered by: internal time\n",
            "   List reduced from 370 to 100 due to restriction <100>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.031    0.031    0.032    0.032 series.py:4496(explode)\n",
            "        1    0.028    0.028    0.029    0.029 pandas_compat.py:755(table_to_dataframe)\n",
            "        1    0.015    0.015    0.016    0.016 core.py:1389(read)\n",
            "        1    0.006    0.006    0.006    0.006 {built-in method _collections._count_elements}\n",
            "        1    0.003    0.003    0.003    0.003 heapq.py:521(nlargest)\n",
            "        1    0.002    0.002    0.002    0.002 missing.py:305(_isna_string_dtype)\n",
            "        1    0.002    0.002    0.002    0.002 core.py:1243(__init__)\n",
            "        1    0.002    0.002    0.002    0.002 {method 'tolist' of 'numpy.ndarray' objects}\n",
            "        1    0.001    0.001    0.095    0.095 q3_time.py:5(q3_time)\n",
            "        1    0.001    0.001    0.001    0.001 {method 'repeat' of 'numpy.ndarray' objects}\n",
            "        1    0.000    0.000    0.001    0.001 take.py:120(_take_nd_ndarray)\n",
            "        2    0.000    0.000    0.000    0.000 cast.py:1157(maybe_infer_to_datetimelike)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
            "        5    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
            "        3    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
            "        2    0.000    0.000    0.000    0.000 decoder.py:343(raw_decode)\n",
            "        1    0.000    0.000    0.000    0.000 utils.py:239(maybe_convert_indices)\n",
            "  342/338    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
            "        6    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method nt.stat}\n",
            "        3    0.000    0.000    0.000    0.000 numeric.py:300(full)\n",
            "        1    0.000    0.000    0.000    0.000 {built-in method _operator.eq}\n",
            "        3    0.000    0.000    0.000    0.000 base.py:475(__new__)\n",
            "        6    0.000    0.000    0.001    0.000 construction.py:517(sanitize_array)\n",
            "        1    0.000    0.000    0.047    0.047 parquet.py:239(read)\n",
            "        3    0.000    0.000    0.001    0.000 series.py:389(__init__)\n",
            "   107/75    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
            "        3    0.000    0.000    0.002    0.001 frame.py:4062(__getitem__)\n",
            "        5    0.000    0.000    0.000    0.000 base.py:649(_simple_new)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:339(__init__)\n",
            "        7    0.000    0.000    0.000    0.000 generic.py:278(__init__)\n",
            "        5    0.000    0.000    0.000    0.000 generic.py:6236(__finalize__)\n",
            "        1    0.000    0.000    0.000    0.000 astype.py:56(_astype_nansafe)\n",
            "        2    0.000    0.000    0.000    0.000 managers.py:1012(iget)\n",
            "       13    0.000    0.000    0.000    0.000 generic.py:6301(__setattr__)\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:124(maybe_convert_platform)\n",
            "        5    0.000    0.000    0.000    0.000 common.py:1596(pandas_dtype)\n",
            "        1    0.000    0.000    0.047    0.047 parquet.py:498(read_parquet)\n",
            "        1    0.000    0.000    0.001    0.001 pandas_compat.py:886(_deserialize_column_index)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1122(_add_any_metadata)\n",
            "       44    0.000    0.000    0.000    0.000 generic.py:42(_instancecheck)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1056(_reconstruct_columns_from_metadata)\n",
            "       44    0.000    0.000    0.000    0.000 generic.py:37(_check)\n",
            "        1    0.000    0.000    0.000    0.000 common.py:664(get_handle)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:818(_get_extension_dtypes)\n",
            "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedReader' objects}\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:919(_reconstruct_index)\n",
            "       72    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
            "       11    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
            "        1    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
            "        2    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
            "        1    0.000    0.000    0.000    0.000 _optional.py:85(import_optional_dependency)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:1085(<listcomp>)\n",
            "        1    0.000    0.000    0.000    0.000 api.py:39(make_block)\n",
            "     10/9    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
            "        5    0.000    0.000    0.000    0.000 config.py:127(_get_single_key)\n",
            "        1    0.000    0.000    0.000    0.000 pandas_compat.py:888(<dictcomp>)\n",
            "        1    0.000    0.000    0.000    0.000 range.py:1148(take)\n",
            "        1    0.000    0.000    0.001    0.001 base.py:1238(repeat)\n",
            "        3    0.000    0.000    0.000    0.000 managers.py:1863(from_array)\n",
            "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1002(_find_and_load)\n",
            "        2    0.000    0.000    0.002    0.001 missing.py:184(_isna)\n",
            "        7    0.000    0.000    0.000    0.000 base.py:7688(maybe_extract_name)\n",
            "        1    0.000    0.000    0.017    0.017 core.py:1745(read_table)\n",
            "        1    0.000    0.000    0.000    0.000 range.py:137(__new__)\n",
            "       15    0.000    0.000    0.000    0.000 inference.py:334(is_hashable)\n",
            "        1    0.000    0.000    0.000    0.000 base.py:842(_engine)\n",
            "        6    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n",
            "        1    0.000    0.000    0.000    0.000 base.py:1045(astype)\n",
            "        1    0.000    0.000    0.000    0.000 _validators.py:168(validate_args_and_kwargs)\n",
            "       11    0.000    0.000    0.000    0.000 construction.py:481(ensure_wrapped_if_datetimelike)\n",
            "        2    0.000    0.000    0.000    0.000 common.py:231(asarray_tuplesafe)\n",
            "        4    0.000    0.000    0.000    0.000 base.py:5373(__getitem__)\n",
            "        5    0.000    0.000    0.000    0.000 config.py:635(_get_root)\n",
            "       72    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
            "        2    0.000    0.000    0.000    0.000 frame.py:4626(_get_item_cache)\n",
            "        2    0.000    0.000    0.000    0.000 decoder.py:332(decode)\n",
            "        2    0.000    0.000    0.000    0.000 base.py:5323(__contains__)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:520(_cmpkey)\n",
            "        1    0.000    0.000    0.000    0.000 common.py:304(_get_filepath_or_buffer)\n",
            "        5    0.000    0.000    0.000    0.000 config.py:145(_get_option)\n",
            "        4    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:1580(construct_1d_object_array_from_listlike)\n",
            "       18    0.000    0.000    0.000    0.000 range.py:999(__len__)\n",
            "        1    0.000    0.000    0.003    0.003 __init__.py:600(most_common)\n",
            "        1    0.000    0.000    0.001    0.001 frame.py:4130(_getitem_bool_array)\n",
            "        1    0.000    0.000    0.001    0.001 generic.py:4027(take)\n",
            "        1    0.000    0.000    0.000    0.000 base.py:2313(is_unique)\n",
            "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:166(_get_module_lock)\n",
            "        1    0.000    0.000    0.001    0.001 managers.py:623(reindex_indexer)\n",
            "        1    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
            "        7    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
            "        5    0.000    0.000    0.000    0.000 generic.py:6284(__getattr__)\n",
            "        1    0.000    0.000    0.000    0.000 cast.py:551(maybe_promote)\n",
            "        2    0.000    0.000    0.000    0.000 frame.py:3983(_ixs)\n",
            "        3    0.000    0.000    0.000    0.000 managers.py:180(blknos)\n",
            "        9    0.000    0.000    0.000    0.000 managers.py:1993(dtype)\n",
            "        2    0.000    0.000    0.000    0.000 __init__.py:299(loads)\n",
            "        1    0.000    0.000    0.000    0.000 parquet.py:85(_get_path_or_handle)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('narendramodi', 2415),\n",
              " ('Kisanektamorcha', 1942),\n",
              " ('RakeshTikaitBKU', 1723),\n",
              " ('PMOIndia', 1489),\n",
              " ('RahulGandhi', 1218),\n",
              " ('GretaThunberg', 1105),\n",
              " ('RaviSinghKA', 1062),\n",
              " ('rihanna', 1037),\n",
              " ('UNHumanRights', 989),\n",
              " ('meenaharris', 975)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%%script false --no-raise-error\n",
        "from q3_time import q3_time\n",
        "\n",
        "with cProfile.Profile() as pr:\n",
        "    q3_time_result = q3_time(f\"{parquet_path}\")\n",
        "\n",
        "q3_stats = pstats.Stats(pr).strip_dirs().sort_stats('time')\n",
        "q3_stats.dump_stats('./stats/q3_time')\n",
        "q3_stats.print_stats(max_print_stat)\n",
        "q3_time_result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
